{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJwk2osTp-Ed"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIb6iU5PqhvU"
      },
      "source": [
        "##create torch dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVqpaxm-qfli"
      },
      "outputs": [],
      "source": [
        "class KeypointsDataset(Dataset):\n",
        "    def __init__(self, img_dir, data_file):\n",
        "        self.img_dir = img_dir\n",
        "        with open(data_file, \"r\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img = cv2.imread(f\"{self.img_dir}/{item['id']}.png\")\n",
        "        h,w = img.shape[:2]\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = self.transforms(img)\n",
        "        kps = np.array(item['kps']).flatten()\n",
        "        kps = kps.astype(np.float32)\n",
        "\n",
        "        kps[::2] *= 224.0 / w # Adjust x coordinates\n",
        "        kps[1::2] *= 224.0 / h # Adjust y coordinates\n",
        "\n",
        "        return img, kps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93vpOSBJvJ1Q"
      },
      "outputs": [],
      "source": [
        "train_dataset = KeypointsDataset(\"/content/drive/MyDrive/data/images\",\"/content/drive/MyDrive/data/data_train.json\")\n",
        "val_dataset = KeypointsDataset(\"/content/drive/MyDrive/data/images\",\"/content/drive/MyDrive/data/data_val.json\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsufNmUWp7CF"
      },
      "outputs": [],
      "source": [
        "# Limit the training dataset to 4000 images\n",
        "num_train_samples = 3500\n",
        "train_indices = np.random.choice(len(train_dataset), num_train_samples, replace=False)\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "\n",
        "# DataLoader for the subset of training data\n",
        "train_loader = DataLoader(train_subset, batch_size=10, shuffle=True)\n",
        "\n",
        "# DataLoader for the validation data\n",
        "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUyL6Xbtv7NV"
      },
      "source": [
        "##Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w406PPVov526",
        "outputId": "b0cca1c1-8a0a-41a5-ede1-a509a1989465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 119MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "model.fc =torch.nn.Linear(model.fc.in_features\n",
        "                          ,14*2) #replae last layer of network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9egcxFFw_iE"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "390lT0p324bd",
        "outputId": "e63cd2b1-c1b9-46a2-f6fb-9f22cacde1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 15 06:51:09 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8voBvvsxdWy",
        "outputId": "eb675c9a-e16d-4142-c60d-e236352e63e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, iter 0, loss 15006.146484375\n",
            "epoch 0, iter 10, loss 15009.3359375\n",
            "epoch 0, iter 20, loss 14618.376953125\n",
            "epoch 0, iter 30, loss 13294.185546875\n",
            "epoch 0, iter 40, loss 13578.8515625\n",
            "epoch 0, iter 50, loss 13313.326171875\n",
            "epoch 0, iter 60, loss 12738.8271484375\n",
            "epoch 0, iter 70, loss 12499.541015625\n",
            "epoch 0, iter 80, loss 12431.49609375\n",
            "epoch 0, iter 90, loss 10849.7490234375\n",
            "epoch 0, iter 100, loss 11141.986328125\n",
            "epoch 0, iter 110, loss 11423.27734375\n",
            "epoch 0, iter 120, loss 10364.384765625\n",
            "epoch 0, iter 130, loss 9735.759765625\n",
            "epoch 0, iter 140, loss 9658.318359375\n",
            "epoch 0, iter 150, loss 9522.8955078125\n",
            "epoch 0, iter 160, loss 8881.044921875\n",
            "epoch 0, iter 170, loss 7976.72705078125\n",
            "epoch 0, iter 180, loss 8547.12109375\n",
            "epoch 0, iter 190, loss 7885.3759765625\n",
            "epoch 0, iter 200, loss 7717.91357421875\n",
            "epoch 0, iter 210, loss 7657.322265625\n",
            "epoch 0, iter 220, loss 8265.4482421875\n",
            "epoch 0, iter 230, loss 7017.1962890625\n",
            "epoch 0, iter 240, loss 7050.701171875\n",
            "epoch 0, iter 250, loss 6628.451171875\n",
            "epoch 0, iter 260, loss 6141.9716796875\n",
            "epoch 0, iter 270, loss 5728.3740234375\n",
            "epoch 0, iter 280, loss 6103.44580078125\n",
            "epoch 0, iter 290, loss 5490.53271484375\n",
            "epoch 0, iter 300, loss 5182.64990234375\n",
            "epoch 0, iter 310, loss 5223.189453125\n",
            "epoch 0, iter 320, loss 4857.7607421875\n",
            "epoch 0, iter 330, loss 4355.71533203125\n",
            "epoch 0, iter 340, loss 4268.9599609375\n",
            "epoch 1, iter 0, loss 4461.1572265625\n",
            "epoch 1, iter 10, loss 4176.818359375\n",
            "epoch 1, iter 20, loss 4594.7978515625\n",
            "epoch 1, iter 30, loss 3635.946044921875\n",
            "epoch 1, iter 40, loss 3594.9609375\n",
            "epoch 1, iter 50, loss 3085.10888671875\n",
            "epoch 1, iter 60, loss 2979.301513671875\n",
            "epoch 1, iter 70, loss 2847.71435546875\n",
            "epoch 1, iter 80, loss 2993.6875\n",
            "epoch 1, iter 90, loss 2558.713134765625\n",
            "epoch 1, iter 100, loss 2719.76123046875\n",
            "epoch 1, iter 110, loss 2369.05908203125\n",
            "epoch 1, iter 120, loss 2213.853515625\n",
            "epoch 1, iter 130, loss 2225.508056640625\n",
            "epoch 1, iter 140, loss 2156.855712890625\n",
            "epoch 1, iter 150, loss 2077.194580078125\n",
            "epoch 1, iter 160, loss 1801.1390380859375\n",
            "epoch 1, iter 170, loss 2302.658447265625\n",
            "epoch 1, iter 180, loss 1677.62841796875\n",
            "epoch 1, iter 190, loss 1426.80615234375\n",
            "epoch 1, iter 200, loss 1386.2637939453125\n",
            "epoch 1, iter 210, loss 1334.2366943359375\n",
            "epoch 1, iter 220, loss 1616.0408935546875\n",
            "epoch 1, iter 230, loss 1117.0162353515625\n",
            "epoch 1, iter 240, loss 1139.62548828125\n",
            "epoch 1, iter 250, loss 1085.318115234375\n",
            "epoch 1, iter 260, loss 966.0386352539062\n",
            "epoch 1, iter 270, loss 990.1676635742188\n",
            "epoch 1, iter 280, loss 1048.7862548828125\n",
            "epoch 1, iter 290, loss 791.69921875\n",
            "epoch 1, iter 300, loss 795.5233154296875\n",
            "epoch 1, iter 310, loss 670.46435546875\n",
            "epoch 1, iter 320, loss 1305.52197265625\n",
            "epoch 1, iter 330, loss 590.9411010742188\n",
            "epoch 1, iter 340, loss 681.9249877929688\n",
            "epoch 2, iter 0, loss 627.1041259765625\n",
            "epoch 2, iter 10, loss 802.9354248046875\n",
            "epoch 2, iter 20, loss 536.6080932617188\n",
            "epoch 2, iter 30, loss 531.84130859375\n",
            "epoch 2, iter 40, loss 468.9599914550781\n",
            "epoch 2, iter 50, loss 631.389404296875\n",
            "epoch 2, iter 60, loss 430.0316162109375\n",
            "epoch 2, iter 70, loss 593.3938598632812\n",
            "epoch 2, iter 80, loss 315.56182861328125\n",
            "epoch 2, iter 90, loss 331.5680847167969\n",
            "epoch 2, iter 100, loss 296.94647216796875\n",
            "epoch 2, iter 110, loss 262.43939208984375\n",
            "epoch 2, iter 120, loss 175.2332305908203\n",
            "epoch 2, iter 130, loss 352.0630187988281\n",
            "epoch 2, iter 140, loss 305.9222717285156\n",
            "epoch 2, iter 150, loss 295.546630859375\n",
            "epoch 2, iter 160, loss 325.8999328613281\n",
            "epoch 2, iter 170, loss 205.78273010253906\n",
            "epoch 2, iter 180, loss 210.2369842529297\n",
            "epoch 2, iter 190, loss 182.19699096679688\n",
            "epoch 2, iter 200, loss 150.84071350097656\n",
            "epoch 2, iter 210, loss 139.3754119873047\n",
            "epoch 2, iter 220, loss 112.5804443359375\n",
            "epoch 2, iter 230, loss 83.13452911376953\n",
            "epoch 2, iter 240, loss 165.20034790039062\n",
            "epoch 2, iter 250, loss 97.22112274169922\n",
            "epoch 2, iter 260, loss 114.47872161865234\n",
            "epoch 2, iter 270, loss 92.40930938720703\n",
            "epoch 2, iter 280, loss 57.44196319580078\n",
            "epoch 2, iter 290, loss 79.22178649902344\n",
            "epoch 2, iter 300, loss 52.055294036865234\n",
            "epoch 2, iter 310, loss 78.67557525634766\n",
            "epoch 2, iter 320, loss 76.51114654541016\n",
            "epoch 2, iter 330, loss 50.8133430480957\n",
            "epoch 2, iter 340, loss 89.3411865234375\n",
            "epoch 3, iter 0, loss 53.95781326293945\n",
            "epoch 3, iter 10, loss 49.41410827636719\n",
            "epoch 3, iter 20, loss 57.34602737426758\n",
            "epoch 3, iter 30, loss 480.7537841796875\n",
            "epoch 3, iter 40, loss 65.38502502441406\n",
            "epoch 3, iter 50, loss 63.08743667602539\n",
            "epoch 3, iter 60, loss 94.54764556884766\n",
            "epoch 3, iter 70, loss 66.6899642944336\n",
            "epoch 3, iter 80, loss 100.21763610839844\n",
            "epoch 3, iter 90, loss 49.10098648071289\n",
            "epoch 3, iter 100, loss 204.55677795410156\n",
            "epoch 3, iter 110, loss 37.855247497558594\n",
            "epoch 3, iter 120, loss 35.60651779174805\n",
            "epoch 3, iter 130, loss 32.98411560058594\n",
            "epoch 3, iter 140, loss 67.52762603759766\n",
            "epoch 3, iter 150, loss 77.02096557617188\n",
            "epoch 3, iter 160, loss 35.97346878051758\n",
            "epoch 3, iter 170, loss 43.010616302490234\n",
            "epoch 3, iter 180, loss 59.56965637207031\n",
            "epoch 3, iter 190, loss 168.972412109375\n",
            "epoch 3, iter 200, loss 38.84611511230469\n",
            "epoch 3, iter 210, loss 38.11962127685547\n",
            "epoch 3, iter 220, loss 47.77744674682617\n",
            "epoch 3, iter 230, loss 47.607940673828125\n",
            "epoch 3, iter 240, loss 42.364280700683594\n",
            "epoch 3, iter 250, loss 42.03936767578125\n",
            "epoch 3, iter 260, loss 42.2037239074707\n",
            "epoch 3, iter 270, loss 44.47356414794922\n",
            "epoch 3, iter 280, loss 24.902591705322266\n",
            "epoch 3, iter 290, loss 73.87652587890625\n",
            "epoch 3, iter 300, loss 28.971920013427734\n",
            "epoch 3, iter 310, loss 34.288169860839844\n",
            "epoch 3, iter 320, loss 55.321693420410156\n",
            "epoch 3, iter 330, loss 105.13217163085938\n",
            "epoch 3, iter 340, loss 23.76042366027832\n",
            "epoch 4, iter 0, loss 40.3583984375\n",
            "epoch 4, iter 10, loss 43.556861877441406\n",
            "epoch 4, iter 20, loss 41.56466293334961\n",
            "epoch 4, iter 30, loss 37.68356704711914\n",
            "epoch 4, iter 40, loss 42.429073333740234\n",
            "epoch 4, iter 50, loss 69.26555633544922\n",
            "epoch 4, iter 60, loss 40.50410079956055\n",
            "epoch 4, iter 70, loss 75.8414306640625\n",
            "epoch 4, iter 80, loss 68.07086944580078\n",
            "epoch 4, iter 90, loss 37.065853118896484\n",
            "epoch 4, iter 100, loss 40.653804779052734\n",
            "epoch 4, iter 110, loss 56.13690185546875\n",
            "epoch 4, iter 120, loss 43.999576568603516\n",
            "epoch 4, iter 130, loss 23.30202865600586\n",
            "epoch 4, iter 140, loss 79.10519409179688\n",
            "epoch 4, iter 150, loss 94.0399169921875\n",
            "epoch 4, iter 160, loss 47.15740203857422\n",
            "epoch 4, iter 170, loss 29.58700180053711\n",
            "epoch 4, iter 180, loss 36.898555755615234\n",
            "epoch 4, iter 190, loss 51.78365707397461\n",
            "epoch 4, iter 200, loss 33.79750442504883\n",
            "epoch 4, iter 210, loss 32.6016731262207\n",
            "epoch 4, iter 220, loss 26.06684112548828\n",
            "epoch 4, iter 230, loss 24.71599769592285\n",
            "epoch 4, iter 240, loss 35.97336959838867\n",
            "epoch 4, iter 250, loss 35.48515701293945\n",
            "epoch 4, iter 260, loss 28.769001007080078\n",
            "epoch 4, iter 270, loss 25.710054397583008\n",
            "epoch 4, iter 280, loss 62.1512565612793\n",
            "epoch 4, iter 290, loss 34.5797233581543\n",
            "epoch 4, iter 300, loss 43.073150634765625\n",
            "epoch 4, iter 310, loss 82.40564727783203\n",
            "epoch 4, iter 320, loss 33.58566665649414\n",
            "epoch 4, iter 330, loss 34.66087341308594\n",
            "epoch 4, iter 340, loss 28.823911666870117\n",
            "epoch 5, iter 0, loss 53.0286750793457\n",
            "epoch 5, iter 10, loss 42.723785400390625\n",
            "epoch 5, iter 20, loss 29.632442474365234\n",
            "epoch 5, iter 30, loss 44.177154541015625\n",
            "epoch 5, iter 40, loss 37.017948150634766\n",
            "epoch 5, iter 50, loss 31.049274444580078\n",
            "epoch 5, iter 60, loss 39.31968307495117\n",
            "epoch 5, iter 70, loss 135.5266876220703\n",
            "epoch 5, iter 80, loss 39.94990921020508\n",
            "epoch 5, iter 90, loss 72.10013580322266\n",
            "epoch 5, iter 100, loss 37.32102584838867\n",
            "epoch 5, iter 110, loss 64.0423812866211\n",
            "epoch 5, iter 120, loss 28.994701385498047\n",
            "epoch 5, iter 130, loss 80.47998046875\n",
            "epoch 5, iter 140, loss 46.85641098022461\n",
            "epoch 5, iter 150, loss 20.15606117248535\n",
            "epoch 5, iter 160, loss 28.465293884277344\n",
            "epoch 5, iter 170, loss 36.234317779541016\n",
            "epoch 5, iter 180, loss 36.339195251464844\n",
            "epoch 5, iter 190, loss 35.0420036315918\n",
            "epoch 5, iter 200, loss 35.1363410949707\n",
            "epoch 5, iter 210, loss 353.38873291015625\n",
            "epoch 5, iter 220, loss 31.21257972717285\n",
            "epoch 5, iter 230, loss 42.002037048339844\n",
            "epoch 5, iter 240, loss 33.559539794921875\n",
            "epoch 5, iter 250, loss 22.708572387695312\n",
            "epoch 5, iter 260, loss 20.160125732421875\n",
            "epoch 5, iter 270, loss 54.62538146972656\n",
            "epoch 5, iter 280, loss 42.93216323852539\n",
            "epoch 5, iter 290, loss 31.205957412719727\n",
            "epoch 5, iter 300, loss 40.59346389770508\n",
            "epoch 5, iter 310, loss 22.402738571166992\n",
            "epoch 5, iter 320, loss 25.881053924560547\n",
            "epoch 5, iter 330, loss 44.59064483642578\n",
            "epoch 5, iter 340, loss 28.63421630859375\n",
            "epoch 6, iter 0, loss 32.10921096801758\n",
            "epoch 6, iter 10, loss 22.479324340820312\n",
            "epoch 6, iter 20, loss 24.117023468017578\n",
            "epoch 6, iter 30, loss 12.337782859802246\n",
            "epoch 6, iter 40, loss 35.81357955932617\n",
            "epoch 6, iter 50, loss 41.434017181396484\n",
            "epoch 6, iter 60, loss 25.420364379882812\n",
            "epoch 6, iter 70, loss 21.80071258544922\n",
            "epoch 6, iter 80, loss 25.143903732299805\n",
            "epoch 6, iter 90, loss 31.80829429626465\n",
            "epoch 6, iter 100, loss 91.98751831054688\n",
            "epoch 6, iter 110, loss 25.582569122314453\n",
            "epoch 6, iter 120, loss 16.750511169433594\n",
            "epoch 6, iter 130, loss 21.40357780456543\n",
            "epoch 6, iter 140, loss 26.52978515625\n",
            "epoch 6, iter 150, loss 34.24358367919922\n",
            "epoch 6, iter 160, loss 37.82967758178711\n",
            "epoch 6, iter 170, loss 24.77690887451172\n",
            "epoch 6, iter 180, loss 65.52074432373047\n",
            "epoch 6, iter 190, loss 41.78240966796875\n",
            "epoch 6, iter 200, loss 85.80441284179688\n",
            "epoch 6, iter 210, loss 18.721952438354492\n",
            "epoch 6, iter 220, loss 20.87908363342285\n",
            "epoch 6, iter 230, loss 27.190649032592773\n",
            "epoch 6, iter 240, loss 22.176359176635742\n",
            "epoch 6, iter 250, loss 25.677583694458008\n",
            "epoch 6, iter 260, loss 32.394012451171875\n",
            "epoch 6, iter 270, loss 49.9075813293457\n",
            "epoch 6, iter 280, loss 90.97489929199219\n",
            "epoch 6, iter 290, loss 16.199071884155273\n",
            "epoch 6, iter 300, loss 23.029191970825195\n",
            "epoch 6, iter 310, loss 17.38664436340332\n",
            "epoch 6, iter 320, loss 27.674211502075195\n",
            "epoch 6, iter 330, loss 57.13645553588867\n",
            "epoch 6, iter 340, loss 37.93699264526367\n",
            "epoch 7, iter 0, loss 21.714914321899414\n",
            "epoch 7, iter 10, loss 23.848257064819336\n",
            "epoch 7, iter 20, loss 77.4050521850586\n",
            "epoch 7, iter 30, loss 19.376548767089844\n",
            "epoch 7, iter 40, loss 23.665287017822266\n",
            "epoch 7, iter 50, loss 47.46488571166992\n",
            "epoch 7, iter 60, loss 32.48807144165039\n",
            "epoch 7, iter 70, loss 16.236135482788086\n",
            "epoch 7, iter 80, loss 19.748748779296875\n",
            "epoch 7, iter 90, loss 35.85567855834961\n",
            "epoch 7, iter 100, loss 24.76239585876465\n",
            "epoch 7, iter 110, loss 14.050993919372559\n",
            "epoch 7, iter 120, loss 10.72182559967041\n",
            "epoch 7, iter 130, loss 128.45677185058594\n",
            "epoch 7, iter 140, loss 28.437665939331055\n",
            "epoch 7, iter 150, loss 14.392147064208984\n",
            "epoch 7, iter 160, loss 74.89993286132812\n",
            "epoch 7, iter 170, loss 15.996603012084961\n",
            "epoch 7, iter 180, loss 24.038768768310547\n",
            "epoch 7, iter 190, loss 28.862258911132812\n",
            "epoch 7, iter 200, loss 27.132532119750977\n",
            "epoch 7, iter 210, loss 69.7604751586914\n",
            "epoch 7, iter 220, loss 23.68842315673828\n",
            "epoch 7, iter 230, loss 26.246742248535156\n",
            "epoch 7, iter 240, loss 29.13530921936035\n",
            "epoch 7, iter 250, loss 34.00791931152344\n",
            "epoch 7, iter 260, loss 16.902551651000977\n",
            "epoch 7, iter 270, loss 26.977710723876953\n",
            "epoch 7, iter 280, loss 12.751808166503906\n",
            "epoch 7, iter 290, loss 11.30756664276123\n",
            "epoch 7, iter 300, loss 22.936174392700195\n",
            "epoch 7, iter 310, loss 16.561819076538086\n",
            "epoch 7, iter 320, loss 19.094806671142578\n",
            "epoch 7, iter 330, loss 12.245856285095215\n",
            "epoch 7, iter 340, loss 22.066593170166016\n",
            "epoch 8, iter 0, loss 15.221899032592773\n",
            "epoch 8, iter 10, loss 13.18697738647461\n",
            "epoch 8, iter 20, loss 18.351093292236328\n",
            "epoch 8, iter 30, loss 39.13164520263672\n",
            "epoch 8, iter 40, loss 42.97185516357422\n",
            "epoch 8, iter 50, loss 14.758245468139648\n",
            "epoch 8, iter 60, loss 133.60276794433594\n",
            "epoch 8, iter 70, loss 20.33493995666504\n",
            "epoch 8, iter 80, loss 14.506031036376953\n",
            "epoch 8, iter 90, loss 14.624937057495117\n",
            "epoch 8, iter 100, loss 16.668020248413086\n",
            "epoch 8, iter 110, loss 24.85096549987793\n",
            "epoch 8, iter 120, loss 90.67778015136719\n",
            "epoch 8, iter 130, loss 58.11078643798828\n",
            "epoch 8, iter 140, loss 22.756019592285156\n",
            "epoch 8, iter 150, loss 6.811388969421387\n",
            "epoch 8, iter 160, loss 9.64857292175293\n",
            "epoch 8, iter 170, loss 34.92537307739258\n",
            "epoch 8, iter 180, loss 120.31678009033203\n",
            "epoch 8, iter 190, loss 13.381919860839844\n",
            "epoch 8, iter 200, loss 38.744991302490234\n",
            "epoch 8, iter 210, loss 6.839618682861328\n",
            "epoch 8, iter 220, loss 124.30340576171875\n",
            "epoch 8, iter 230, loss 16.268320083618164\n",
            "epoch 8, iter 240, loss 9.710444450378418\n",
            "epoch 8, iter 250, loss 16.824180603027344\n",
            "epoch 8, iter 260, loss 21.497533798217773\n",
            "epoch 8, iter 270, loss 10.920077323913574\n",
            "epoch 8, iter 280, loss 21.658950805664062\n",
            "epoch 8, iter 290, loss 17.220468521118164\n",
            "epoch 8, iter 300, loss 15.607817649841309\n",
            "epoch 8, iter 310, loss 16.807559967041016\n",
            "epoch 8, iter 320, loss 10.317191123962402\n",
            "epoch 8, iter 330, loss 19.930856704711914\n",
            "epoch 8, iter 340, loss 13.271891593933105\n",
            "epoch 9, iter 0, loss 50.870941162109375\n",
            "epoch 9, iter 10, loss 40.66082000732422\n",
            "epoch 9, iter 20, loss 5.639455795288086\n",
            "epoch 9, iter 30, loss 8.055473327636719\n",
            "epoch 9, iter 40, loss 20.382230758666992\n",
            "epoch 9, iter 50, loss 10.727579116821289\n",
            "epoch 9, iter 60, loss 6.723923206329346\n",
            "epoch 9, iter 70, loss 8.378005981445312\n",
            "epoch 9, iter 80, loss 15.317913055419922\n",
            "epoch 9, iter 90, loss 18.790029525756836\n",
            "epoch 9, iter 100, loss 10.103841781616211\n",
            "epoch 9, iter 110, loss 51.08867645263672\n",
            "epoch 9, iter 120, loss 20.475313186645508\n",
            "epoch 9, iter 130, loss 12.774480819702148\n",
            "epoch 9, iter 140, loss 7.863309860229492\n",
            "epoch 9, iter 150, loss 7.322363376617432\n",
            "epoch 9, iter 160, loss 4.958423614501953\n",
            "epoch 9, iter 170, loss 23.82342529296875\n",
            "epoch 9, iter 180, loss 4.626120567321777\n",
            "epoch 9, iter 190, loss 6.356166839599609\n",
            "epoch 9, iter 200, loss 4.926642417907715\n",
            "epoch 9, iter 210, loss 20.080739974975586\n",
            "epoch 9, iter 220, loss 61.18240737915039\n",
            "epoch 9, iter 230, loss 7.919402122497559\n",
            "epoch 9, iter 240, loss 4.195190906524658\n",
            "epoch 9, iter 250, loss 25.24905776977539\n",
            "epoch 9, iter 260, loss 13.05627155303955\n",
            "epoch 9, iter 270, loss 12.357090950012207\n",
            "epoch 9, iter 280, loss 14.400672912597656\n",
            "epoch 9, iter 290, loss 10.461570739746094\n",
            "epoch 9, iter 300, loss 4.528085231781006\n",
            "epoch 9, iter 310, loss 10.586562156677246\n",
            "epoch 9, iter 320, loss 6.061914920806885\n",
            "epoch 9, iter 330, loss 6.679037570953369\n",
            "epoch 9, iter 340, loss 5.604776859283447\n",
            "epoch 10, iter 0, loss 7.227447986602783\n",
            "epoch 10, iter 10, loss 7.738557815551758\n",
            "epoch 10, iter 20, loss 10.894289016723633\n",
            "epoch 10, iter 30, loss 6.828517436981201\n",
            "epoch 10, iter 40, loss 21.111743927001953\n",
            "epoch 10, iter 50, loss 11.666415214538574\n",
            "epoch 10, iter 60, loss 8.455183029174805\n",
            "epoch 10, iter 70, loss 8.495430946350098\n",
            "epoch 10, iter 80, loss 10.24639892578125\n",
            "epoch 10, iter 90, loss 21.819183349609375\n",
            "epoch 10, iter 100, loss 10.510382652282715\n",
            "epoch 10, iter 110, loss 24.449670791625977\n",
            "epoch 10, iter 120, loss 8.066252708435059\n",
            "epoch 10, iter 130, loss 32.68376159667969\n",
            "epoch 10, iter 140, loss 5.91940450668335\n",
            "epoch 10, iter 150, loss 2.4679043292999268\n",
            "epoch 10, iter 160, loss 11.420220375061035\n",
            "epoch 10, iter 170, loss 5.778994560241699\n",
            "epoch 10, iter 180, loss 7.804335117340088\n",
            "epoch 10, iter 190, loss 34.80228805541992\n",
            "epoch 10, iter 200, loss 60.820655822753906\n",
            "epoch 10, iter 210, loss 14.399179458618164\n",
            "epoch 10, iter 220, loss 12.886653900146484\n",
            "epoch 10, iter 230, loss 12.072724342346191\n",
            "epoch 10, iter 240, loss 4.103781700134277\n",
            "epoch 10, iter 250, loss 4.505206108093262\n",
            "epoch 10, iter 260, loss 10.926562309265137\n",
            "epoch 10, iter 270, loss 7.061461925506592\n",
            "epoch 10, iter 280, loss 9.286043167114258\n",
            "epoch 10, iter 290, loss 7.677133560180664\n",
            "epoch 10, iter 300, loss 5.183048725128174\n",
            "epoch 10, iter 310, loss 5.698391437530518\n",
            "epoch 10, iter 320, loss 13.87118148803711\n",
            "epoch 10, iter 330, loss 14.818687438964844\n",
            "epoch 10, iter 340, loss 70.75534057617188\n",
            "epoch 11, iter 0, loss 13.73829174041748\n",
            "epoch 11, iter 10, loss 5.118020057678223\n",
            "epoch 11, iter 20, loss 7.070540904998779\n",
            "epoch 11, iter 30, loss 3.953624725341797\n",
            "epoch 11, iter 40, loss 5.989433765411377\n",
            "epoch 11, iter 50, loss 9.22228717803955\n",
            "epoch 11, iter 60, loss 6.702223777770996\n",
            "epoch 11, iter 70, loss 12.656717300415039\n",
            "epoch 11, iter 80, loss 4.564553260803223\n",
            "epoch 11, iter 90, loss 6.413873195648193\n",
            "epoch 11, iter 100, loss 5.415339946746826\n",
            "epoch 11, iter 110, loss 6.731907367706299\n",
            "epoch 11, iter 120, loss 4.0243730545043945\n",
            "epoch 11, iter 130, loss 15.647283554077148\n",
            "epoch 11, iter 140, loss 14.76101303100586\n",
            "epoch 11, iter 150, loss 16.335554122924805\n",
            "epoch 11, iter 160, loss 42.78517150878906\n",
            "epoch 11, iter 170, loss 10.775668144226074\n",
            "epoch 11, iter 180, loss 11.310257911682129\n",
            "epoch 11, iter 190, loss 6.6175127029418945\n",
            "epoch 11, iter 200, loss 4.68288516998291\n",
            "epoch 11, iter 210, loss 10.097773551940918\n",
            "epoch 11, iter 220, loss 77.10931396484375\n",
            "epoch 11, iter 230, loss 16.873952865600586\n",
            "epoch 11, iter 240, loss 7.797525405883789\n",
            "epoch 11, iter 250, loss 4.661140441894531\n",
            "epoch 11, iter 260, loss 4.203368186950684\n",
            "epoch 11, iter 270, loss 17.128141403198242\n",
            "epoch 11, iter 280, loss 27.068374633789062\n",
            "epoch 11, iter 290, loss 5.172308444976807\n",
            "epoch 11, iter 300, loss 4.622245788574219\n",
            "epoch 11, iter 310, loss 9.396489143371582\n",
            "epoch 11, iter 320, loss 69.03602600097656\n",
            "epoch 11, iter 330, loss 17.335731506347656\n",
            "epoch 11, iter 340, loss 6.338778972625732\n",
            "epoch 12, iter 0, loss 7.7146382331848145\n",
            "epoch 12, iter 10, loss 3.2426233291625977\n",
            "epoch 12, iter 20, loss 3.9283957481384277\n",
            "epoch 12, iter 30, loss 3.4787368774414062\n",
            "epoch 12, iter 40, loss 4.971771717071533\n",
            "epoch 12, iter 50, loss 6.669888973236084\n",
            "epoch 12, iter 60, loss 6.7529988288879395\n",
            "epoch 12, iter 70, loss 18.20706558227539\n",
            "epoch 12, iter 80, loss 11.733871459960938\n",
            "epoch 12, iter 90, loss 7.163262844085693\n",
            "epoch 12, iter 100, loss 4.874925136566162\n",
            "epoch 12, iter 110, loss 19.57695960998535\n",
            "epoch 12, iter 120, loss 10.840154647827148\n",
            "epoch 12, iter 130, loss 3.2730538845062256\n",
            "epoch 12, iter 140, loss 22.992494583129883\n",
            "epoch 12, iter 150, loss 11.0914945602417\n",
            "epoch 12, iter 160, loss 6.255636215209961\n",
            "epoch 12, iter 170, loss 7.908336639404297\n",
            "epoch 12, iter 180, loss 83.14225769042969\n",
            "epoch 12, iter 190, loss 6.277327060699463\n",
            "epoch 12, iter 200, loss 44.2753791809082\n",
            "epoch 12, iter 210, loss 6.9277472496032715\n",
            "epoch 12, iter 220, loss 5.757909774780273\n",
            "epoch 12, iter 230, loss 3.3249173164367676\n",
            "epoch 12, iter 240, loss 8.013813018798828\n",
            "epoch 12, iter 250, loss 4.713624000549316\n",
            "epoch 12, iter 260, loss 43.65233612060547\n",
            "epoch 12, iter 270, loss 6.650373935699463\n",
            "epoch 12, iter 280, loss 3.625241994857788\n",
            "epoch 12, iter 290, loss 4.764301300048828\n",
            "epoch 12, iter 300, loss 4.754273891448975\n",
            "epoch 12, iter 310, loss 17.862520217895508\n",
            "epoch 12, iter 320, loss 7.784732341766357\n",
            "epoch 12, iter 330, loss 6.077610015869141\n",
            "epoch 12, iter 340, loss 9.389115333557129\n",
            "epoch 13, iter 0, loss 23.455385208129883\n",
            "epoch 13, iter 10, loss 3.7086687088012695\n",
            "epoch 13, iter 20, loss 6.0994553565979\n",
            "epoch 13, iter 30, loss 3.306410074234009\n",
            "epoch 13, iter 40, loss 5.642251491546631\n",
            "epoch 13, iter 50, loss 6.580120086669922\n",
            "epoch 13, iter 60, loss 4.9256062507629395\n",
            "epoch 13, iter 70, loss 3.2836191654205322\n",
            "epoch 13, iter 80, loss 6.854766845703125\n",
            "epoch 13, iter 90, loss 6.25213098526001\n",
            "epoch 13, iter 100, loss 13.120733261108398\n",
            "epoch 13, iter 110, loss 2.5735647678375244\n",
            "epoch 13, iter 120, loss 5.741161346435547\n",
            "epoch 13, iter 130, loss 106.70317077636719\n",
            "epoch 13, iter 140, loss 21.13131332397461\n",
            "epoch 13, iter 150, loss 6.094326496124268\n",
            "epoch 13, iter 160, loss 2.921767473220825\n",
            "epoch 13, iter 170, loss 12.646644592285156\n",
            "epoch 13, iter 180, loss 5.752686500549316\n",
            "epoch 13, iter 190, loss 1.7696077823638916\n",
            "epoch 13, iter 200, loss 1.7446422576904297\n",
            "epoch 13, iter 210, loss 2.417703628540039\n",
            "epoch 13, iter 220, loss 16.994619369506836\n",
            "epoch 13, iter 230, loss 3.2856740951538086\n",
            "epoch 13, iter 240, loss 9.535320281982422\n",
            "epoch 13, iter 250, loss 29.930688858032227\n",
            "epoch 13, iter 260, loss 43.2718391418457\n",
            "epoch 13, iter 270, loss 3.0359580516815186\n",
            "epoch 13, iter 280, loss 4.942599773406982\n",
            "epoch 13, iter 290, loss 10.341304779052734\n",
            "epoch 13, iter 300, loss 2.088683605194092\n",
            "epoch 13, iter 310, loss 306.6912536621094\n",
            "epoch 13, iter 320, loss 8.865354537963867\n",
            "epoch 13, iter 330, loss 34.688385009765625\n",
            "epoch 13, iter 340, loss 5.320937156677246\n",
            "epoch 14, iter 0, loss 14.145374298095703\n",
            "epoch 14, iter 10, loss 4.3733367919921875\n",
            "epoch 14, iter 20, loss 5.252877712249756\n",
            "epoch 14, iter 30, loss 6.835464000701904\n",
            "epoch 14, iter 40, loss 19.443267822265625\n",
            "epoch 14, iter 50, loss 2.9276764392852783\n",
            "epoch 14, iter 60, loss 7.356719017028809\n",
            "epoch 14, iter 70, loss 15.078707695007324\n",
            "epoch 14, iter 80, loss 8.341224670410156\n",
            "epoch 14, iter 90, loss 6.575035095214844\n",
            "epoch 14, iter 100, loss 8.674510955810547\n",
            "epoch 14, iter 110, loss 5.233644485473633\n",
            "epoch 14, iter 120, loss 4.280285358428955\n",
            "epoch 14, iter 130, loss 4.181702613830566\n",
            "epoch 14, iter 140, loss 5.178384304046631\n",
            "epoch 14, iter 150, loss 6.272363185882568\n",
            "epoch 14, iter 160, loss 2.6685118675231934\n",
            "epoch 14, iter 170, loss 31.371776580810547\n",
            "epoch 14, iter 180, loss 4.513604640960693\n",
            "epoch 14, iter 190, loss 2.492947816848755\n",
            "epoch 14, iter 200, loss 5.861588001251221\n",
            "epoch 14, iter 210, loss 6.479118824005127\n",
            "epoch 14, iter 220, loss 15.186098098754883\n",
            "epoch 14, iter 230, loss 9.378917694091797\n",
            "epoch 14, iter 240, loss 4.63756799697876\n",
            "epoch 14, iter 250, loss 12.054560661315918\n",
            "epoch 14, iter 260, loss 2.4724082946777344\n",
            "epoch 14, iter 270, loss 7.841307640075684\n",
            "epoch 14, iter 280, loss 4.3818182945251465\n",
            "epoch 14, iter 290, loss 3.1286230087280273\n",
            "epoch 14, iter 300, loss 3.2757489681243896\n",
            "epoch 14, iter 310, loss 3.661846160888672\n",
            "epoch 14, iter 320, loss 7.8286213874816895\n",
            "epoch 14, iter 330, loss 7.587739944458008\n",
            "epoch 14, iter 340, loss 4.704419136047363\n",
            "epoch 15, iter 0, loss 2.2584376335144043\n",
            "epoch 15, iter 10, loss 1.505903720855713\n",
            "epoch 15, iter 20, loss 1.5712584257125854\n",
            "epoch 15, iter 30, loss 2.3757851123809814\n",
            "epoch 15, iter 40, loss 4.727195739746094\n",
            "epoch 15, iter 50, loss 88.63252258300781\n",
            "epoch 15, iter 60, loss 4.6754560470581055\n",
            "epoch 15, iter 70, loss 3.1193833351135254\n",
            "epoch 15, iter 80, loss 3.5695981979370117\n",
            "epoch 15, iter 90, loss 2.6529757976531982\n",
            "epoch 15, iter 100, loss 2.9904589653015137\n",
            "epoch 15, iter 110, loss 3.9970180988311768\n",
            "epoch 15, iter 120, loss 2.2564687728881836\n",
            "epoch 15, iter 130, loss 13.878510475158691\n",
            "epoch 15, iter 140, loss 3.025960922241211\n",
            "epoch 15, iter 150, loss 5.18753719329834\n",
            "epoch 15, iter 160, loss 4.673062801361084\n",
            "epoch 15, iter 170, loss 19.396366119384766\n",
            "epoch 15, iter 180, loss 9.295333862304688\n",
            "epoch 15, iter 190, loss 6.712507247924805\n",
            "epoch 15, iter 200, loss 7.613801002502441\n",
            "epoch 15, iter 210, loss 7.015890121459961\n",
            "epoch 15, iter 220, loss 2.5952389240264893\n",
            "epoch 15, iter 230, loss 2.8542320728302\n",
            "epoch 15, iter 240, loss 2.5188050270080566\n",
            "epoch 15, iter 250, loss 57.724769592285156\n",
            "epoch 15, iter 260, loss 3.373854875564575\n",
            "epoch 15, iter 270, loss 23.80570411682129\n",
            "epoch 15, iter 280, loss 30.941329956054688\n",
            "epoch 15, iter 290, loss 7.0919880867004395\n",
            "epoch 15, iter 300, loss 4.068211555480957\n",
            "epoch 15, iter 310, loss 12.868199348449707\n",
            "epoch 15, iter 320, loss 1.8213720321655273\n",
            "epoch 15, iter 330, loss 16.209272384643555\n",
            "epoch 15, iter 340, loss 67.47541809082031\n",
            "epoch 16, iter 0, loss 1.8725085258483887\n",
            "epoch 16, iter 10, loss 3.7207250595092773\n",
            "epoch 16, iter 20, loss 2.6058156490325928\n",
            "epoch 16, iter 30, loss 3.7347421646118164\n",
            "epoch 16, iter 40, loss 16.92089080810547\n",
            "epoch 16, iter 50, loss 4.214254379272461\n",
            "epoch 16, iter 60, loss 6.485894203186035\n",
            "epoch 16, iter 70, loss 2.8129265308380127\n",
            "epoch 16, iter 80, loss 1.7269721031188965\n",
            "epoch 16, iter 90, loss 9.843216896057129\n",
            "epoch 16, iter 100, loss 3.0150980949401855\n",
            "epoch 16, iter 110, loss 3.826890707015991\n",
            "epoch 16, iter 120, loss 5.370668411254883\n",
            "epoch 16, iter 130, loss 0.8854727745056152\n",
            "epoch 16, iter 140, loss 1.5970991849899292\n",
            "epoch 16, iter 150, loss 1.199076771736145\n",
            "epoch 16, iter 160, loss 2.161895513534546\n",
            "epoch 16, iter 170, loss 18.283203125\n",
            "epoch 16, iter 180, loss 3.835097312927246\n",
            "epoch 16, iter 190, loss 4.587116718292236\n",
            "epoch 16, iter 200, loss 7.55571985244751\n",
            "epoch 16, iter 210, loss 5.448122501373291\n",
            "epoch 16, iter 220, loss 5.3450751304626465\n",
            "epoch 16, iter 230, loss 1.1370145082473755\n",
            "epoch 16, iter 240, loss 3.1485774517059326\n",
            "epoch 16, iter 250, loss 2.0137221813201904\n",
            "epoch 16, iter 260, loss 16.673830032348633\n",
            "epoch 16, iter 270, loss 3.9983198642730713\n",
            "epoch 16, iter 280, loss 8.203707695007324\n",
            "epoch 16, iter 290, loss 4.373323440551758\n",
            "epoch 16, iter 300, loss 3.1316962242126465\n",
            "epoch 16, iter 310, loss 2.940582513809204\n",
            "epoch 16, iter 320, loss 11.341835975646973\n",
            "epoch 16, iter 330, loss 3.9844701290130615\n",
            "epoch 16, iter 340, loss 4.143512725830078\n",
            "epoch 17, iter 0, loss 5.299222946166992\n",
            "epoch 17, iter 10, loss 5.307802200317383\n",
            "epoch 17, iter 20, loss 65.68930053710938\n",
            "epoch 17, iter 30, loss 20.870939254760742\n",
            "epoch 17, iter 40, loss 0.907753586769104\n",
            "epoch 17, iter 50, loss 1.9769439697265625\n",
            "epoch 17, iter 60, loss 3.067094326019287\n",
            "epoch 17, iter 70, loss 1.4278619289398193\n",
            "epoch 17, iter 80, loss 2.1629750728607178\n",
            "epoch 17, iter 90, loss 0.953077495098114\n",
            "epoch 17, iter 100, loss 15.928921699523926\n",
            "epoch 17, iter 110, loss 1.9508743286132812\n",
            "epoch 17, iter 120, loss 4.153853893280029\n",
            "epoch 17, iter 130, loss 1.650152564048767\n",
            "epoch 17, iter 140, loss 1.8735272884368896\n",
            "epoch 17, iter 150, loss 2.8400542736053467\n",
            "epoch 17, iter 160, loss 4.433681488037109\n",
            "epoch 17, iter 170, loss 5.568294048309326\n",
            "epoch 17, iter 180, loss 2.7771291732788086\n",
            "epoch 17, iter 190, loss 6.978163242340088\n",
            "epoch 17, iter 200, loss 3.376105308532715\n",
            "epoch 17, iter 210, loss 7.832849025726318\n",
            "epoch 17, iter 220, loss 1.7731318473815918\n",
            "epoch 17, iter 230, loss 7.076625347137451\n",
            "epoch 17, iter 240, loss 1.380696177482605\n",
            "epoch 17, iter 250, loss 4.397779941558838\n",
            "epoch 17, iter 260, loss 2.7581918239593506\n",
            "epoch 17, iter 270, loss 1.9173965454101562\n",
            "epoch 17, iter 280, loss 7.184165954589844\n",
            "epoch 17, iter 290, loss 5.130943775177002\n",
            "epoch 17, iter 300, loss 9.01894474029541\n",
            "epoch 17, iter 310, loss 3.1262593269348145\n",
            "epoch 17, iter 320, loss 2.434589385986328\n",
            "epoch 17, iter 330, loss 2.475058078765869\n",
            "epoch 17, iter 340, loss 31.68926429748535\n",
            "epoch 18, iter 0, loss 5.761618614196777\n",
            "epoch 18, iter 10, loss 2.352611780166626\n",
            "epoch 18, iter 20, loss 3.053795337677002\n",
            "epoch 18, iter 30, loss 3.0157620906829834\n",
            "epoch 18, iter 40, loss 4.8947038650512695\n",
            "epoch 18, iter 50, loss 3.949875831604004\n",
            "epoch 18, iter 60, loss 3.530994176864624\n",
            "epoch 18, iter 70, loss 8.413952827453613\n",
            "epoch 18, iter 80, loss 5.808830738067627\n",
            "epoch 18, iter 90, loss 61.138572692871094\n",
            "epoch 18, iter 100, loss 2.771984100341797\n",
            "epoch 18, iter 110, loss 9.144268035888672\n",
            "epoch 18, iter 120, loss 7.787717819213867\n",
            "epoch 18, iter 130, loss 3.685204267501831\n",
            "epoch 18, iter 140, loss 5.338931083679199\n",
            "epoch 18, iter 150, loss 0.8335645794868469\n",
            "epoch 18, iter 160, loss 2.986884832382202\n",
            "epoch 18, iter 170, loss 2.5823490619659424\n",
            "epoch 18, iter 180, loss 9.827146530151367\n",
            "epoch 18, iter 190, loss 2.301630973815918\n",
            "epoch 18, iter 200, loss 5.245426654815674\n",
            "epoch 18, iter 210, loss 1.4573256969451904\n",
            "epoch 18, iter 220, loss 10.905517578125\n",
            "epoch 18, iter 230, loss 1.7743474245071411\n",
            "epoch 18, iter 240, loss 2.2030551433563232\n",
            "epoch 18, iter 250, loss 2.6034886837005615\n",
            "epoch 18, iter 260, loss 1.4502376317977905\n",
            "epoch 18, iter 270, loss 5.638792514801025\n",
            "epoch 18, iter 280, loss 3.080037832260132\n",
            "epoch 18, iter 290, loss 3.495938301086426\n",
            "epoch 18, iter 300, loss 3.566957712173462\n",
            "epoch 18, iter 310, loss 4.1783857345581055\n",
            "epoch 18, iter 320, loss 6.943545341491699\n",
            "epoch 18, iter 330, loss 1.569967269897461\n",
            "epoch 18, iter 340, loss 3.02728009223938\n",
            "epoch 19, iter 0, loss 2.920121669769287\n",
            "epoch 19, iter 10, loss 0.9248620271682739\n",
            "epoch 19, iter 20, loss 1.3473899364471436\n",
            "epoch 19, iter 30, loss 11.571784019470215\n",
            "epoch 19, iter 40, loss 8.617060661315918\n",
            "epoch 19, iter 50, loss 2.5379722118377686\n",
            "epoch 19, iter 60, loss 2.2222750186920166\n",
            "epoch 19, iter 70, loss 2.3855578899383545\n",
            "epoch 19, iter 80, loss 3.671875476837158\n",
            "epoch 19, iter 90, loss 2.806201219558716\n",
            "epoch 19, iter 100, loss 3.868858575820923\n",
            "epoch 19, iter 110, loss 24.57126235961914\n",
            "epoch 19, iter 120, loss 1.8066567182540894\n",
            "epoch 19, iter 130, loss 1.7193455696105957\n",
            "epoch 19, iter 140, loss 7.901212215423584\n",
            "epoch 19, iter 150, loss 3.8228681087493896\n",
            "epoch 19, iter 160, loss 18.744121551513672\n",
            "epoch 19, iter 170, loss 8.915984153747559\n",
            "epoch 19, iter 180, loss 3.967327117919922\n",
            "epoch 19, iter 190, loss 10.065443992614746\n",
            "epoch 19, iter 200, loss 3.8582911491394043\n",
            "epoch 19, iter 210, loss 58.575035095214844\n",
            "epoch 19, iter 220, loss 2.253359794616699\n",
            "epoch 19, iter 230, loss 57.59903335571289\n",
            "epoch 19, iter 240, loss 4.2036614418029785\n",
            "epoch 19, iter 250, loss 1.5437512397766113\n",
            "epoch 19, iter 260, loss 6.5575971603393555\n",
            "epoch 19, iter 270, loss 5.9046406745910645\n",
            "epoch 19, iter 280, loss 4.559732913970947\n",
            "epoch 19, iter 290, loss 4.436905860900879\n",
            "epoch 19, iter 300, loss 64.19593048095703\n",
            "epoch 19, iter 310, loss 1.291290521621704\n",
            "epoch 19, iter 320, loss 2.0845446586608887\n",
            "epoch 19, iter 330, loss 5.148045063018799\n",
            "epoch 19, iter 340, loss 2.6813812255859375\n",
            "epoch 20, iter 0, loss 2.2106552124023438\n",
            "epoch 20, iter 10, loss 3.0428009033203125\n",
            "epoch 20, iter 20, loss 4.894393444061279\n",
            "epoch 20, iter 30, loss 0.8141466379165649\n",
            "epoch 20, iter 40, loss 11.165767669677734\n",
            "epoch 20, iter 50, loss 1.8234699964523315\n",
            "epoch 20, iter 60, loss 2.755126476287842\n",
            "epoch 20, iter 70, loss 5.795555114746094\n",
            "epoch 20, iter 80, loss 3.1683578491210938\n",
            "epoch 20, iter 90, loss 2.6900177001953125\n",
            "epoch 20, iter 100, loss 1.7427215576171875\n",
            "epoch 20, iter 110, loss 17.1220703125\n",
            "epoch 20, iter 120, loss 4.354896068572998\n",
            "epoch 20, iter 130, loss 2.2579991817474365\n",
            "epoch 20, iter 140, loss 1.2656102180480957\n",
            "epoch 20, iter 150, loss 5.009509086608887\n",
            "epoch 20, iter 160, loss 5.720818519592285\n",
            "epoch 20, iter 170, loss 1.5345945358276367\n",
            "epoch 20, iter 180, loss 2.6804425716400146\n",
            "epoch 20, iter 190, loss 6.132984161376953\n",
            "epoch 20, iter 200, loss 1.851379632949829\n",
            "epoch 20, iter 210, loss 0.7620656490325928\n",
            "epoch 20, iter 220, loss 2.505117893218994\n",
            "epoch 20, iter 230, loss 1.0214401483535767\n",
            "epoch 20, iter 240, loss 3.7481541633605957\n",
            "epoch 20, iter 250, loss 2.701812267303467\n",
            "epoch 20, iter 260, loss 3.8614165782928467\n",
            "epoch 20, iter 270, loss 6.014367580413818\n",
            "epoch 20, iter 280, loss 14.27442455291748\n",
            "epoch 20, iter 290, loss 52.239158630371094\n",
            "epoch 20, iter 300, loss 2.4283182621002197\n",
            "epoch 20, iter 310, loss 49.069820404052734\n",
            "epoch 20, iter 320, loss 7.576932430267334\n",
            "epoch 20, iter 330, loss 3.953507900238037\n",
            "epoch 20, iter 340, loss 1.3355457782745361\n",
            "epoch 21, iter 0, loss 3.6666202545166016\n",
            "epoch 21, iter 10, loss 5.490518569946289\n",
            "epoch 21, iter 20, loss 1.4783214330673218\n",
            "epoch 21, iter 30, loss 3.1818199157714844\n",
            "epoch 21, iter 40, loss 5.826876640319824\n",
            "epoch 21, iter 50, loss 0.8244536519050598\n",
            "epoch 21, iter 60, loss 3.200680732727051\n",
            "epoch 21, iter 70, loss 1.314150333404541\n",
            "epoch 21, iter 80, loss 5.026061058044434\n",
            "epoch 21, iter 90, loss 1.3374115228652954\n",
            "epoch 21, iter 100, loss 21.715059280395508\n",
            "epoch 21, iter 110, loss 9.990950584411621\n",
            "epoch 21, iter 120, loss 10.077289581298828\n",
            "epoch 21, iter 130, loss 6.826988220214844\n",
            "epoch 21, iter 140, loss 6.208225250244141\n",
            "epoch 21, iter 150, loss 0.823209285736084\n",
            "epoch 21, iter 160, loss 3.824199914932251\n",
            "epoch 21, iter 170, loss 1.9536616802215576\n",
            "epoch 21, iter 180, loss 0.8721761703491211\n",
            "epoch 21, iter 190, loss 3.887275457382202\n",
            "epoch 21, iter 200, loss 2.3580894470214844\n",
            "epoch 21, iter 210, loss 5.127197742462158\n",
            "epoch 21, iter 220, loss 1.3648455142974854\n",
            "epoch 21, iter 230, loss 2.147493839263916\n",
            "epoch 21, iter 240, loss 3.333627462387085\n",
            "epoch 21, iter 250, loss 13.099509239196777\n",
            "epoch 21, iter 260, loss 7.897623062133789\n",
            "epoch 21, iter 270, loss 3.588859796524048\n",
            "epoch 21, iter 280, loss 8.197297096252441\n",
            "epoch 21, iter 290, loss 2.395081043243408\n",
            "epoch 21, iter 300, loss 8.924798011779785\n",
            "epoch 21, iter 310, loss 4.202395439147949\n",
            "epoch 21, iter 320, loss 4.523593425750732\n",
            "epoch 21, iter 330, loss 2.6790151596069336\n",
            "epoch 21, iter 340, loss 2.8073668479919434\n",
            "epoch 22, iter 0, loss 1.0451890230178833\n",
            "epoch 22, iter 10, loss 1.911440134048462\n",
            "epoch 22, iter 20, loss 2.76690673828125\n",
            "epoch 22, iter 30, loss 3.405052423477173\n",
            "epoch 22, iter 40, loss 1.7665973901748657\n",
            "epoch 22, iter 50, loss 7.141574382781982\n",
            "epoch 22, iter 60, loss 1.6746922731399536\n",
            "epoch 22, iter 70, loss 0.8029210567474365\n",
            "epoch 22, iter 80, loss 1.5768096446990967\n",
            "epoch 22, iter 90, loss 0.4738876223564148\n",
            "epoch 22, iter 100, loss 1.8217947483062744\n",
            "epoch 22, iter 110, loss 1.0887922048568726\n",
            "epoch 22, iter 120, loss 1.7147992849349976\n",
            "epoch 22, iter 130, loss 6.787724018096924\n",
            "epoch 22, iter 140, loss 1.4570175409317017\n",
            "epoch 22, iter 150, loss 3.2213058471679688\n",
            "epoch 22, iter 160, loss 16.514156341552734\n",
            "epoch 22, iter 170, loss 46.566673278808594\n",
            "epoch 22, iter 180, loss 1.2734266519546509\n",
            "epoch 22, iter 190, loss 4.112829685211182\n",
            "epoch 22, iter 200, loss 0.6752087473869324\n",
            "epoch 22, iter 210, loss 9.440176963806152\n",
            "epoch 22, iter 220, loss 6.6935811042785645\n",
            "epoch 22, iter 230, loss 0.7006046772003174\n",
            "epoch 22, iter 240, loss 2.753363847732544\n",
            "epoch 22, iter 250, loss 1.0841763019561768\n",
            "epoch 22, iter 260, loss 2.0489742755889893\n",
            "epoch 22, iter 270, loss 1.7277276515960693\n",
            "epoch 22, iter 280, loss 2.73066782951355\n",
            "epoch 22, iter 290, loss 0.6589476466178894\n",
            "epoch 22, iter 300, loss 5.513148784637451\n",
            "epoch 22, iter 310, loss 87.37470245361328\n",
            "epoch 22, iter 320, loss 9.222612380981445\n",
            "epoch 22, iter 330, loss 2.9315285682678223\n",
            "epoch 22, iter 340, loss 3.264199733734131\n",
            "epoch 23, iter 0, loss 4.8728227615356445\n",
            "epoch 23, iter 10, loss 1.1640838384628296\n",
            "epoch 23, iter 20, loss 1.8490182161331177\n",
            "epoch 23, iter 30, loss 1.3172001838684082\n",
            "epoch 23, iter 40, loss 8.366037368774414\n",
            "epoch 23, iter 50, loss 5.254708766937256\n",
            "epoch 23, iter 60, loss 3.5661160945892334\n",
            "epoch 23, iter 70, loss 2.636992931365967\n",
            "epoch 23, iter 80, loss 2.538294792175293\n",
            "epoch 23, iter 90, loss 13.710373878479004\n",
            "epoch 23, iter 100, loss 3.9607911109924316\n",
            "epoch 23, iter 110, loss 5.415363788604736\n",
            "epoch 23, iter 120, loss 1.4449864625930786\n",
            "epoch 23, iter 130, loss 0.7488058805465698\n",
            "epoch 23, iter 140, loss 3.1078262329101562\n",
            "epoch 23, iter 150, loss 1.5826054811477661\n",
            "epoch 23, iter 160, loss 1.0973308086395264\n",
            "epoch 23, iter 170, loss 1.8206565380096436\n",
            "epoch 23, iter 180, loss 2.529731035232544\n",
            "epoch 23, iter 190, loss 1.6039056777954102\n",
            "epoch 23, iter 200, loss 3.083378314971924\n",
            "epoch 23, iter 210, loss 5.00354528427124\n",
            "epoch 23, iter 220, loss 4.2483930587768555\n",
            "epoch 23, iter 230, loss 3.258857488632202\n",
            "epoch 23, iter 240, loss 0.7448714375495911\n",
            "epoch 23, iter 250, loss 0.4601600170135498\n",
            "epoch 23, iter 260, loss 6.040605068206787\n",
            "epoch 23, iter 270, loss 3.7558681964874268\n",
            "epoch 23, iter 280, loss 13.075885772705078\n",
            "epoch 23, iter 290, loss 1.9988163709640503\n",
            "epoch 23, iter 300, loss 2.11380934715271\n",
            "epoch 23, iter 310, loss 2.322833299636841\n",
            "epoch 23, iter 320, loss 1.987176537513733\n",
            "epoch 23, iter 330, loss 2.1304357051849365\n",
            "epoch 23, iter 340, loss 0.8736867308616638\n",
            "epoch 24, iter 0, loss 2.1494085788726807\n",
            "epoch 24, iter 10, loss 1.7051680088043213\n",
            "epoch 24, iter 20, loss 0.744632363319397\n",
            "epoch 24, iter 30, loss 2.0143628120422363\n",
            "epoch 24, iter 40, loss 16.685590744018555\n",
            "epoch 24, iter 50, loss 3.911424160003662\n",
            "epoch 24, iter 60, loss 1.7524385452270508\n",
            "epoch 24, iter 70, loss 15.484113693237305\n",
            "epoch 24, iter 80, loss 3.6924328804016113\n",
            "epoch 24, iter 90, loss 2.2888219356536865\n",
            "epoch 24, iter 100, loss 4.3785505294799805\n",
            "epoch 24, iter 110, loss 1.3492985963821411\n",
            "epoch 24, iter 120, loss 5.067343711853027\n",
            "epoch 24, iter 130, loss 2.5775272846221924\n",
            "epoch 24, iter 140, loss 1.2949053049087524\n",
            "epoch 24, iter 150, loss 23.938051223754883\n",
            "epoch 24, iter 160, loss 2.421985387802124\n",
            "epoch 24, iter 170, loss 1.0594340562820435\n",
            "epoch 24, iter 180, loss 0.9285445213317871\n",
            "epoch 24, iter 190, loss 1.6905486583709717\n",
            "epoch 24, iter 200, loss 10.69545841217041\n",
            "epoch 24, iter 210, loss 1.718497395515442\n",
            "epoch 24, iter 220, loss 0.8681759238243103\n",
            "epoch 24, iter 230, loss 3.978013515472412\n",
            "epoch 24, iter 240, loss 1.1687484979629517\n",
            "epoch 24, iter 250, loss 2.409616708755493\n",
            "epoch 24, iter 260, loss 1.8362553119659424\n",
            "epoch 24, iter 270, loss 3.2263224124908447\n",
            "epoch 24, iter 280, loss 11.77416706085205\n",
            "epoch 24, iter 290, loss 2.720531702041626\n",
            "epoch 24, iter 300, loss 1.0887173414230347\n",
            "epoch 24, iter 310, loss 4.793938636779785\n",
            "epoch 24, iter 320, loss 5.423768997192383\n",
            "epoch 24, iter 330, loss 60.063232421875\n",
            "epoch 24, iter 340, loss 1.8233363628387451\n",
            "epoch 25, iter 0, loss 1.5113747119903564\n",
            "epoch 25, iter 10, loss 2.1940128803253174\n",
            "epoch 25, iter 20, loss 4.078727722167969\n",
            "epoch 25, iter 30, loss 2.152604103088379\n",
            "epoch 25, iter 40, loss 3.839909553527832\n",
            "epoch 25, iter 50, loss 2.6136505603790283\n",
            "epoch 25, iter 60, loss 1.6241501569747925\n",
            "epoch 25, iter 70, loss 5.002376079559326\n",
            "epoch 25, iter 80, loss 1.6095138788223267\n",
            "epoch 25, iter 90, loss 2.122875928878784\n",
            "epoch 25, iter 100, loss 17.023977279663086\n",
            "epoch 25, iter 110, loss 4.400579929351807\n",
            "epoch 25, iter 120, loss 2.292320728302002\n",
            "epoch 25, iter 130, loss 4.802173614501953\n",
            "epoch 25, iter 140, loss 1.6243664026260376\n",
            "epoch 25, iter 150, loss 1.6865332126617432\n",
            "epoch 25, iter 160, loss 1.4040993452072144\n",
            "epoch 25, iter 170, loss 1.5273792743682861\n",
            "epoch 25, iter 180, loss 6.101606845855713\n",
            "epoch 25, iter 190, loss 20.074132919311523\n",
            "epoch 25, iter 200, loss 1.735080599784851\n",
            "epoch 25, iter 210, loss 2.3198726177215576\n",
            "epoch 25, iter 220, loss 3.3990750312805176\n",
            "epoch 25, iter 230, loss 2.361957550048828\n",
            "epoch 25, iter 240, loss 1.0467820167541504\n",
            "epoch 25, iter 250, loss 5.586478233337402\n",
            "epoch 25, iter 260, loss 3.9351797103881836\n",
            "epoch 25, iter 270, loss 1.8733572959899902\n",
            "epoch 25, iter 280, loss 1.898771047592163\n",
            "epoch 25, iter 290, loss 2.250771999359131\n",
            "epoch 25, iter 300, loss 9.340433120727539\n",
            "epoch 25, iter 310, loss 3.446641683578491\n",
            "epoch 25, iter 320, loss 1.4471261501312256\n",
            "epoch 25, iter 330, loss 1.520890474319458\n",
            "epoch 25, iter 340, loss 2.780564546585083\n",
            "epoch 26, iter 0, loss 4.859060764312744\n",
            "epoch 26, iter 10, loss 1.1579838991165161\n",
            "epoch 26, iter 20, loss 1.7214305400848389\n",
            "epoch 26, iter 30, loss 2.7077410221099854\n",
            "epoch 26, iter 40, loss 1.71795654296875\n",
            "epoch 26, iter 50, loss 1.132500171661377\n",
            "epoch 26, iter 60, loss 4.759007930755615\n",
            "epoch 26, iter 70, loss 1.5594632625579834\n",
            "epoch 26, iter 80, loss 0.8176640272140503\n",
            "epoch 26, iter 90, loss 2.3935723304748535\n",
            "epoch 26, iter 100, loss 1.2936091423034668\n",
            "epoch 26, iter 110, loss 1.4219260215759277\n",
            "epoch 26, iter 120, loss 8.920022964477539\n",
            "epoch 26, iter 130, loss 1.863215446472168\n",
            "epoch 26, iter 140, loss 4.923730850219727\n",
            "epoch 26, iter 150, loss 5.673018455505371\n",
            "epoch 26, iter 160, loss 7.081159591674805\n",
            "epoch 26, iter 170, loss 6.046175003051758\n",
            "epoch 26, iter 180, loss 5.023030757904053\n",
            "epoch 26, iter 190, loss 189.4518585205078\n",
            "epoch 26, iter 200, loss 1.709465742111206\n",
            "epoch 26, iter 210, loss 1.757392406463623\n",
            "epoch 26, iter 220, loss 1.204984426498413\n",
            "epoch 26, iter 230, loss 2.3266637325286865\n",
            "epoch 26, iter 240, loss 0.8698277473449707\n",
            "epoch 26, iter 250, loss 4.308262348175049\n",
            "epoch 26, iter 260, loss 4.128537654876709\n",
            "epoch 26, iter 270, loss 3.7018415927886963\n",
            "epoch 26, iter 280, loss 2.5939526557922363\n",
            "epoch 26, iter 290, loss 5.238502025604248\n",
            "epoch 26, iter 300, loss 3.1098287105560303\n",
            "epoch 26, iter 310, loss 3.4748218059539795\n",
            "epoch 26, iter 320, loss 1.3523164987564087\n",
            "epoch 26, iter 330, loss 2.4562900066375732\n",
            "epoch 26, iter 340, loss 62.62718963623047\n",
            "epoch 27, iter 0, loss 2.6397287845611572\n",
            "epoch 27, iter 10, loss 4.041277885437012\n",
            "epoch 27, iter 20, loss 0.7853128910064697\n",
            "epoch 27, iter 30, loss 2.2154173851013184\n",
            "epoch 27, iter 40, loss 4.577084541320801\n",
            "epoch 27, iter 50, loss 0.8100605607032776\n",
            "epoch 27, iter 60, loss 5.322545528411865\n",
            "epoch 27, iter 70, loss 0.8553575873374939\n",
            "epoch 27, iter 80, loss 1.0813931226730347\n",
            "epoch 27, iter 90, loss 1.9648939371109009\n",
            "epoch 27, iter 100, loss 0.8218470215797424\n",
            "epoch 27, iter 110, loss 1.655936360359192\n",
            "epoch 27, iter 120, loss 2.540426015853882\n",
            "epoch 27, iter 130, loss 2.3154990673065186\n",
            "epoch 27, iter 140, loss 0.7576053142547607\n",
            "epoch 27, iter 150, loss 0.48695340752601624\n",
            "epoch 27, iter 160, loss 1.2984422445297241\n",
            "epoch 27, iter 170, loss 3.878371238708496\n",
            "epoch 27, iter 180, loss 3.987384557723999\n",
            "epoch 27, iter 190, loss 2.445007562637329\n",
            "epoch 27, iter 200, loss 2.1646156311035156\n",
            "epoch 27, iter 210, loss 2.943554162979126\n",
            "epoch 27, iter 220, loss 2.958966016769409\n",
            "epoch 27, iter 230, loss 1.5500785112380981\n",
            "epoch 27, iter 240, loss 16.795135498046875\n",
            "epoch 27, iter 250, loss 2.117206335067749\n",
            "epoch 27, iter 260, loss 2.3250203132629395\n",
            "epoch 27, iter 270, loss 0.7768653631210327\n",
            "epoch 27, iter 280, loss 3.401719808578491\n",
            "epoch 27, iter 290, loss 36.402061462402344\n",
            "epoch 27, iter 300, loss 5.040010929107666\n",
            "epoch 27, iter 310, loss 0.5845546126365662\n",
            "epoch 27, iter 320, loss 1.7370878458023071\n",
            "epoch 27, iter 330, loss 2.3705968856811523\n",
            "epoch 27, iter 340, loss 2.205428123474121\n",
            "epoch 28, iter 0, loss 1.794097661972046\n",
            "epoch 28, iter 10, loss 6.924542427062988\n",
            "epoch 28, iter 20, loss 0.7654539942741394\n",
            "epoch 28, iter 30, loss 1.8751133680343628\n",
            "epoch 28, iter 40, loss 1.6160025596618652\n",
            "epoch 28, iter 50, loss 3.143294334411621\n",
            "epoch 28, iter 60, loss 4.014605522155762\n",
            "epoch 28, iter 70, loss 1.6955753564834595\n",
            "epoch 28, iter 80, loss 23.293670654296875\n",
            "epoch 28, iter 90, loss 3.577406167984009\n",
            "epoch 28, iter 100, loss 0.645399808883667\n",
            "epoch 28, iter 110, loss 2.8762502670288086\n",
            "epoch 28, iter 120, loss 2.0460829734802246\n",
            "epoch 28, iter 130, loss 0.7464417815208435\n",
            "epoch 28, iter 140, loss 27.438800811767578\n",
            "epoch 28, iter 150, loss 4.259139537811279\n",
            "epoch 28, iter 160, loss 1.140995979309082\n",
            "epoch 28, iter 170, loss 1.5663622617721558\n",
            "epoch 28, iter 180, loss 2.740367889404297\n",
            "epoch 28, iter 190, loss 0.7461743354797363\n",
            "epoch 28, iter 200, loss 1.9386746883392334\n",
            "epoch 28, iter 210, loss 2.027848243713379\n",
            "epoch 28, iter 220, loss 1.2945090532302856\n",
            "epoch 28, iter 230, loss 0.8770633339881897\n",
            "epoch 28, iter 240, loss 2.708012819290161\n",
            "epoch 28, iter 250, loss 0.8027595281600952\n",
            "epoch 28, iter 260, loss 2.021468162536621\n",
            "epoch 28, iter 270, loss 0.6887299418449402\n",
            "epoch 28, iter 280, loss 3.7864222526550293\n",
            "epoch 28, iter 290, loss 1.6875014305114746\n",
            "epoch 28, iter 300, loss 0.5365675091743469\n",
            "epoch 28, iter 310, loss 0.46577683091163635\n",
            "epoch 28, iter 320, loss 3.8326663970947266\n",
            "epoch 28, iter 330, loss 1.2461949586868286\n",
            "epoch 28, iter 340, loss 4.604069232940674\n",
            "epoch 29, iter 0, loss 1.6587820053100586\n",
            "epoch 29, iter 10, loss 1.339134693145752\n",
            "epoch 29, iter 20, loss 1.0876861810684204\n",
            "epoch 29, iter 30, loss 1.3506909608840942\n",
            "epoch 29, iter 40, loss 3.480085611343384\n",
            "epoch 29, iter 50, loss 2.240614652633667\n",
            "epoch 29, iter 60, loss 2.849435329437256\n",
            "epoch 29, iter 70, loss 2.287515878677368\n",
            "epoch 29, iter 80, loss 3.2725677490234375\n",
            "epoch 29, iter 90, loss 1.7146512269973755\n",
            "epoch 29, iter 100, loss 1.4075971841812134\n",
            "epoch 29, iter 110, loss 1.6680437326431274\n",
            "epoch 29, iter 120, loss 2.6453235149383545\n",
            "epoch 29, iter 130, loss 1.676953911781311\n",
            "epoch 29, iter 140, loss 38.045982360839844\n",
            "epoch 29, iter 150, loss 3.1933350563049316\n",
            "epoch 29, iter 160, loss 2.649566173553467\n",
            "epoch 29, iter 170, loss 1.9145673513412476\n",
            "epoch 29, iter 180, loss 1.082602620124817\n",
            "epoch 29, iter 190, loss 2.1454408168792725\n",
            "epoch 29, iter 200, loss 2.3457953929901123\n",
            "epoch 29, iter 210, loss 16.242666244506836\n",
            "epoch 29, iter 220, loss 2.9544920921325684\n",
            "epoch 29, iter 230, loss 0.8682001233100891\n",
            "epoch 29, iter 240, loss 1.8042997121810913\n",
            "epoch 29, iter 250, loss 2.6536216735839844\n",
            "epoch 29, iter 260, loss 9.135844230651855\n",
            "epoch 29, iter 270, loss 2.6627190113067627\n",
            "epoch 29, iter 280, loss 61.887611389160156\n",
            "epoch 29, iter 290, loss 0.8979098200798035\n",
            "epoch 29, iter 300, loss 0.5625038146972656\n",
            "epoch 29, iter 310, loss 2.2717878818511963\n",
            "epoch 29, iter 320, loss 0.9124422669410706\n",
            "epoch 29, iter 330, loss 0.6080811023712158\n",
            "epoch 29, iter 340, loss 0.6898966431617737\n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "try:\n",
        "    for epoch in range(epochs):\n",
        "        for i, (imgs, kps) in enumerate(train_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            kps = kps.to(device)\n",
        "            model.to(device)  # Move the model to the device\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, kps)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"epoch {epoch}, iter {i}, loss {loss.item()}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # Save the model if the program gets interrupted\n",
        "    torch.save(model.state_dict(), 'interrupted_model.pth')\n",
        "    print(\"Model saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp7NDz5y7bXj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "g1UVq1-0y25t"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"keypoints_model.pth\")\n",
        "torch.save(optimizer.state_dict(), 'optimizer.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r5QnQLv8ZnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8555c20e-bfa1-4a99-b7c2-7212e7255c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30, iter 0, loss 1.03144371509552\n",
            "epoch 30, iter 10, loss 1.8771153688430786\n",
            "epoch 30, iter 20, loss 2.3323233127593994\n",
            "epoch 30, iter 30, loss 1.1387273073196411\n",
            "epoch 30, iter 40, loss 0.64808589220047\n",
            "epoch 30, iter 50, loss 0.9844887852668762\n",
            "epoch 30, iter 60, loss 1.197916030883789\n",
            "epoch 30, iter 70, loss 0.8513360023498535\n",
            "epoch 30, iter 80, loss 0.6063991189002991\n",
            "epoch 30, iter 90, loss 2.442007541656494\n",
            "epoch 30, iter 100, loss 1.3507256507873535\n",
            "epoch 30, iter 110, loss 1.092253565788269\n",
            "epoch 30, iter 120, loss 1.5133079290390015\n",
            "epoch 30, iter 130, loss 1.1405905485153198\n",
            "epoch 30, iter 140, loss 3.0844662189483643\n",
            "epoch 30, iter 150, loss 10.970961570739746\n",
            "epoch 30, iter 160, loss 3.5040664672851562\n",
            "epoch 30, iter 170, loss 0.8491945266723633\n",
            "epoch 30, iter 180, loss 1.4908809661865234\n",
            "epoch 30, iter 190, loss 1.735029697418213\n",
            "epoch 30, iter 200, loss 0.6444419622421265\n",
            "epoch 30, iter 210, loss 0.8996357321739197\n",
            "epoch 30, iter 220, loss 1.1706624031066895\n",
            "epoch 30, iter 230, loss 0.777949333190918\n",
            "epoch 30, iter 240, loss 2.291400194168091\n",
            "epoch 30, iter 250, loss 0.7172876596450806\n",
            "epoch 30, iter 260, loss 1.2805442810058594\n",
            "epoch 30, iter 270, loss 4.701136112213135\n",
            "epoch 30, iter 280, loss 1.0283960103988647\n",
            "epoch 30, iter 290, loss 0.851162850856781\n",
            "epoch 30, iter 300, loss 21.88294219970703\n",
            "epoch 30, iter 310, loss 4.979049205780029\n",
            "epoch 30, iter 320, loss 1.5824058055877686\n",
            "epoch 30, iter 330, loss 4.579005718231201\n",
            "epoch 30, iter 340, loss 2.4718668460845947\n",
            "epoch 31, iter 0, loss 1.4886947870254517\n",
            "epoch 31, iter 10, loss 1.2919113636016846\n",
            "epoch 31, iter 20, loss 1.0808788537979126\n",
            "epoch 31, iter 30, loss 1.644903302192688\n",
            "epoch 31, iter 40, loss 2.174541711807251\n",
            "epoch 31, iter 50, loss 2.9637107849121094\n",
            "epoch 31, iter 60, loss 1.0196014642715454\n",
            "epoch 31, iter 70, loss 1.109529733657837\n",
            "epoch 31, iter 80, loss 0.9405289888381958\n",
            "epoch 31, iter 90, loss 1.642914891242981\n",
            "epoch 31, iter 100, loss 2.019388198852539\n",
            "epoch 31, iter 110, loss 92.1966552734375\n",
            "epoch 31, iter 120, loss 2.340015411376953\n",
            "epoch 31, iter 130, loss 1.1286816596984863\n",
            "epoch 31, iter 140, loss 2.295238733291626\n",
            "epoch 31, iter 150, loss 3.222440481185913\n",
            "epoch 31, iter 160, loss 3.099745988845825\n",
            "epoch 31, iter 170, loss 1.8749442100524902\n",
            "epoch 31, iter 180, loss 2.5599913597106934\n",
            "epoch 31, iter 190, loss 3.723597526550293\n",
            "epoch 31, iter 200, loss 3.2683732509613037\n",
            "epoch 31, iter 210, loss 1.1477806568145752\n",
            "epoch 31, iter 220, loss 1.27987802028656\n",
            "epoch 31, iter 230, loss 1.9871013164520264\n",
            "epoch 31, iter 240, loss 1.819265604019165\n",
            "epoch 31, iter 250, loss 2.0635809898376465\n",
            "epoch 31, iter 260, loss 2.289048910140991\n",
            "epoch 31, iter 270, loss 2.7812554836273193\n",
            "epoch 31, iter 280, loss 1.8227205276489258\n",
            "epoch 31, iter 290, loss 1.6648776531219482\n",
            "epoch 31, iter 300, loss 3.6812548637390137\n",
            "epoch 31, iter 310, loss 1.7028043270111084\n",
            "epoch 31, iter 320, loss 4.963201999664307\n",
            "epoch 31, iter 330, loss 1.700060248374939\n",
            "epoch 31, iter 340, loss 0.5244545340538025\n",
            "epoch 32, iter 0, loss 0.7496970295906067\n",
            "epoch 32, iter 10, loss 1.4426298141479492\n",
            "epoch 32, iter 20, loss 2.3342843055725098\n",
            "epoch 32, iter 30, loss 0.6673632264137268\n",
            "epoch 32, iter 40, loss 3.443580389022827\n",
            "epoch 32, iter 50, loss 1.246224045753479\n",
            "epoch 32, iter 60, loss 1.8639318943023682\n",
            "epoch 32, iter 70, loss 1.462766408920288\n",
            "epoch 32, iter 80, loss 1.2565170526504517\n",
            "epoch 32, iter 90, loss 2.3189024925231934\n",
            "epoch 32, iter 100, loss 2.7133777141571045\n",
            "epoch 32, iter 110, loss 10.145590782165527\n",
            "epoch 32, iter 120, loss 1.598171591758728\n",
            "epoch 32, iter 130, loss 1.1196726560592651\n",
            "epoch 32, iter 140, loss 1.8740373849868774\n",
            "epoch 32, iter 150, loss 2.018563747406006\n",
            "epoch 32, iter 160, loss 7.662346363067627\n",
            "epoch 32, iter 170, loss 1.790678858757019\n",
            "epoch 32, iter 180, loss 1.138768196105957\n",
            "epoch 32, iter 190, loss 2.4479563236236572\n",
            "epoch 32, iter 200, loss 0.8620482683181763\n",
            "epoch 32, iter 210, loss 1.3805738687515259\n",
            "epoch 32, iter 220, loss 5.6221818923950195\n",
            "epoch 32, iter 230, loss 0.5377963781356812\n",
            "epoch 32, iter 240, loss 0.816606879234314\n",
            "epoch 32, iter 250, loss 0.742218554019928\n",
            "epoch 32, iter 260, loss 2.2846453189849854\n",
            "epoch 32, iter 270, loss 39.859676361083984\n",
            "epoch 32, iter 280, loss 1.5592046976089478\n",
            "epoch 32, iter 290, loss 0.3461902439594269\n",
            "epoch 32, iter 300, loss 2.3271498680114746\n",
            "epoch 32, iter 310, loss 0.8619135022163391\n",
            "epoch 32, iter 320, loss 2.586479902267456\n",
            "epoch 32, iter 330, loss 0.6252979040145874\n",
            "epoch 32, iter 340, loss 0.4923155605792999\n",
            "epoch 33, iter 0, loss 2.5027618408203125\n",
            "epoch 33, iter 10, loss 0.7209590673446655\n",
            "epoch 33, iter 20, loss 2.551689863204956\n",
            "epoch 33, iter 30, loss 1.501418113708496\n",
            "epoch 33, iter 40, loss 2.022223472595215\n",
            "epoch 33, iter 50, loss 2.0997636318206787\n",
            "epoch 33, iter 60, loss 0.5918781757354736\n",
            "epoch 33, iter 70, loss 2.118685007095337\n",
            "epoch 33, iter 80, loss 1.2712187767028809\n",
            "epoch 33, iter 90, loss 0.7451761364936829\n",
            "epoch 33, iter 100, loss 0.8874364495277405\n",
            "epoch 33, iter 110, loss 1.21804940700531\n",
            "epoch 33, iter 120, loss 1.4257404804229736\n",
            "epoch 33, iter 130, loss 2.3833799362182617\n",
            "epoch 33, iter 140, loss 1.3425451517105103\n",
            "epoch 33, iter 150, loss 1.9536219835281372\n",
            "epoch 33, iter 160, loss 1.093682050704956\n",
            "epoch 33, iter 170, loss 1.0614558458328247\n",
            "epoch 33, iter 180, loss 1.6746336221694946\n",
            "epoch 33, iter 190, loss 0.5396605730056763\n",
            "epoch 33, iter 200, loss 1.006261944770813\n",
            "epoch 33, iter 210, loss 2.821990489959717\n",
            "epoch 33, iter 220, loss 3.257061243057251\n",
            "epoch 33, iter 230, loss 0.6482606530189514\n",
            "epoch 33, iter 240, loss 0.6146324872970581\n",
            "epoch 33, iter 250, loss 1.597360610961914\n",
            "epoch 33, iter 260, loss 2.1618411540985107\n",
            "epoch 33, iter 270, loss 2.063619613647461\n",
            "epoch 33, iter 280, loss 0.8536735773086548\n",
            "epoch 33, iter 290, loss 1.4536296129226685\n",
            "epoch 33, iter 300, loss 0.8183950185775757\n",
            "epoch 33, iter 310, loss 0.8767057061195374\n",
            "epoch 33, iter 320, loss 1.5102462768554688\n",
            "epoch 33, iter 330, loss 1.0347802639007568\n",
            "epoch 33, iter 340, loss 1.6299207210540771\n",
            "epoch 34, iter 0, loss 1.8667984008789062\n",
            "epoch 34, iter 10, loss 2.7050302028656006\n",
            "epoch 34, iter 20, loss 1.4272174835205078\n",
            "epoch 34, iter 30, loss 0.8738877773284912\n",
            "epoch 34, iter 40, loss 0.4988184869289398\n",
            "epoch 34, iter 50, loss 1.5757546424865723\n",
            "epoch 34, iter 60, loss 0.6339349150657654\n",
            "epoch 34, iter 70, loss 0.573617696762085\n",
            "epoch 34, iter 80, loss 2.389657497406006\n",
            "epoch 34, iter 90, loss 3.868583917617798\n",
            "epoch 34, iter 100, loss 1.4474550485610962\n",
            "epoch 34, iter 110, loss 1.2502273321151733\n",
            "epoch 34, iter 120, loss 2.8733792304992676\n",
            "epoch 34, iter 130, loss 0.8940086960792542\n",
            "epoch 34, iter 140, loss 0.7995994687080383\n",
            "epoch 34, iter 150, loss 2.4123904705047607\n",
            "epoch 34, iter 160, loss 1.1861296892166138\n",
            "epoch 34, iter 170, loss 0.4816839396953583\n",
            "epoch 34, iter 180, loss 0.9798939228057861\n",
            "epoch 34, iter 190, loss 41.1605224609375\n",
            "epoch 34, iter 200, loss 0.4265891909599304\n",
            "epoch 34, iter 210, loss 2.0461161136627197\n",
            "epoch 34, iter 220, loss 33.732337951660156\n",
            "epoch 34, iter 230, loss 1.2888017892837524\n",
            "epoch 34, iter 240, loss 0.8090537786483765\n",
            "epoch 34, iter 250, loss 18.934589385986328\n",
            "epoch 34, iter 260, loss 2.8454017639160156\n",
            "epoch 34, iter 270, loss 1.355696678161621\n",
            "epoch 34, iter 280, loss 1.636742353439331\n",
            "epoch 34, iter 290, loss 0.9737063050270081\n",
            "epoch 34, iter 300, loss 1.3514107465744019\n",
            "epoch 34, iter 310, loss 0.3638033866882324\n",
            "epoch 34, iter 320, loss 1.7901337146759033\n",
            "epoch 34, iter 330, loss 1.1151384115219116\n",
            "epoch 34, iter 340, loss 1.3814252614974976\n",
            "epoch 35, iter 0, loss 0.5453647375106812\n",
            "epoch 35, iter 10, loss 5.359314918518066\n",
            "epoch 35, iter 20, loss 1.3586210012435913\n",
            "epoch 35, iter 30, loss 1.690422773361206\n",
            "epoch 35, iter 40, loss 1.427663803100586\n",
            "epoch 35, iter 50, loss 0.8528974056243896\n",
            "epoch 35, iter 60, loss 1.9246808290481567\n",
            "epoch 35, iter 70, loss 0.3112940788269043\n",
            "epoch 35, iter 80, loss 1.6834570169448853\n",
            "epoch 35, iter 90, loss 3.075390577316284\n",
            "epoch 35, iter 100, loss 0.9931094646453857\n",
            "epoch 35, iter 110, loss 2.430206775665283\n",
            "epoch 35, iter 120, loss 6.957794666290283\n",
            "epoch 35, iter 130, loss 2.0803048610687256\n",
            "epoch 35, iter 140, loss 1.3835113048553467\n",
            "epoch 35, iter 150, loss 2.537520170211792\n",
            "epoch 35, iter 160, loss 2.896256446838379\n",
            "epoch 35, iter 170, loss 1.5636799335479736\n",
            "epoch 35, iter 180, loss 1.486503005027771\n",
            "epoch 35, iter 190, loss 0.927322268486023\n",
            "epoch 35, iter 200, loss 2.4240527153015137\n",
            "epoch 35, iter 210, loss 2.5156848430633545\n",
            "epoch 35, iter 220, loss 0.5114780068397522\n",
            "epoch 35, iter 230, loss 0.8427716493606567\n",
            "epoch 35, iter 240, loss 1.084346890449524\n",
            "epoch 35, iter 250, loss 0.4748784303665161\n",
            "epoch 35, iter 260, loss 5.875522136688232\n",
            "epoch 35, iter 270, loss 1.4026556015014648\n",
            "epoch 35, iter 280, loss 0.4024476706981659\n",
            "epoch 35, iter 290, loss 1.7299426794052124\n",
            "epoch 35, iter 300, loss 1.6993647813796997\n",
            "epoch 35, iter 310, loss 1.4408748149871826\n",
            "epoch 35, iter 320, loss 1.1886149644851685\n",
            "epoch 35, iter 330, loss 8.930581092834473\n",
            "epoch 35, iter 340, loss 1.500536561012268\n",
            "epoch 36, iter 0, loss 1.2484045028686523\n",
            "epoch 36, iter 10, loss 1.2909966707229614\n",
            "epoch 36, iter 20, loss 135.76931762695312\n",
            "epoch 36, iter 30, loss 1.237003207206726\n",
            "epoch 36, iter 40, loss 2.820723295211792\n",
            "epoch 36, iter 50, loss 3.89377498626709\n",
            "epoch 36, iter 60, loss 1.2926199436187744\n",
            "epoch 36, iter 70, loss 1.2344807386398315\n",
            "epoch 36, iter 80, loss 1.2278963327407837\n",
            "epoch 36, iter 90, loss 1.0051016807556152\n",
            "epoch 36, iter 100, loss 0.784467339515686\n",
            "epoch 36, iter 110, loss 0.2548287510871887\n",
            "epoch 36, iter 120, loss 4.146182537078857\n",
            "epoch 36, iter 130, loss 1.3232537508010864\n",
            "epoch 36, iter 140, loss 1.1060289144515991\n",
            "epoch 36, iter 150, loss 0.5886348485946655\n",
            "epoch 36, iter 160, loss 0.8427091240882874\n",
            "epoch 36, iter 170, loss 0.3820132911205292\n",
            "epoch 36, iter 180, loss 0.7305194735527039\n",
            "epoch 36, iter 190, loss 0.6087217330932617\n",
            "epoch 36, iter 200, loss 2.3318912982940674\n",
            "epoch 36, iter 210, loss 2.9520788192749023\n",
            "epoch 36, iter 220, loss 1.4877829551696777\n",
            "epoch 36, iter 230, loss 0.8306552171707153\n",
            "epoch 36, iter 240, loss 1.0662630796432495\n",
            "epoch 36, iter 250, loss 1.915763258934021\n",
            "epoch 36, iter 260, loss 1.568215012550354\n",
            "epoch 36, iter 270, loss 1.6963380575180054\n",
            "epoch 36, iter 280, loss 0.7572793364524841\n",
            "epoch 36, iter 290, loss 4.6459150314331055\n",
            "epoch 36, iter 300, loss 1.1392490863800049\n",
            "epoch 36, iter 310, loss 0.2901918292045593\n",
            "epoch 36, iter 320, loss 0.44750362634658813\n",
            "epoch 36, iter 330, loss 0.6386059522628784\n",
            "epoch 36, iter 340, loss 1.315786361694336\n",
            "epoch 37, iter 0, loss 1.092697262763977\n",
            "epoch 37, iter 10, loss 0.7176318168640137\n",
            "epoch 37, iter 20, loss 1.0404771566390991\n",
            "epoch 37, iter 30, loss 1.942906379699707\n",
            "epoch 37, iter 40, loss 1.0825681686401367\n",
            "epoch 37, iter 50, loss 1.772904396057129\n",
            "epoch 37, iter 60, loss 0.9980993866920471\n",
            "epoch 37, iter 70, loss 0.8536871671676636\n",
            "epoch 37, iter 80, loss 1.2204954624176025\n",
            "epoch 37, iter 90, loss 0.9940920472145081\n",
            "epoch 37, iter 100, loss 0.7862803339958191\n",
            "epoch 37, iter 110, loss 0.4994703531265259\n",
            "epoch 37, iter 120, loss 0.37626081705093384\n",
            "epoch 37, iter 130, loss 1.0193654298782349\n",
            "epoch 37, iter 140, loss 2.8030636310577393\n",
            "epoch 37, iter 150, loss 0.5497782826423645\n",
            "epoch 37, iter 160, loss 1.4085839986801147\n",
            "epoch 37, iter 170, loss 2.2767369747161865\n",
            "epoch 37, iter 180, loss 3.185378074645996\n",
            "epoch 37, iter 190, loss 2.3096811771392822\n",
            "epoch 37, iter 200, loss 6.387601852416992\n",
            "epoch 37, iter 210, loss 2.128737211227417\n",
            "epoch 37, iter 220, loss 1.5993200540542603\n",
            "epoch 37, iter 230, loss 0.841442883014679\n",
            "epoch 37, iter 240, loss 1.9344316720962524\n",
            "epoch 37, iter 250, loss 0.7312847375869751\n",
            "epoch 37, iter 260, loss 1.4182630777359009\n",
            "epoch 37, iter 270, loss 2.2679030895233154\n",
            "epoch 37, iter 280, loss 1.3853567838668823\n",
            "epoch 37, iter 290, loss 0.9123135805130005\n",
            "epoch 37, iter 300, loss 0.8536662459373474\n",
            "epoch 37, iter 310, loss 137.98330688476562\n",
            "epoch 37, iter 320, loss 2.161771059036255\n",
            "epoch 37, iter 330, loss 4.001463890075684\n",
            "epoch 37, iter 340, loss 0.688357949256897\n",
            "epoch 38, iter 0, loss 0.47434747219085693\n",
            "epoch 38, iter 10, loss 0.9368965029716492\n",
            "epoch 38, iter 20, loss 1.1644784212112427\n",
            "epoch 38, iter 30, loss 1.4899721145629883\n",
            "epoch 38, iter 40, loss 0.3237844705581665\n",
            "epoch 38, iter 50, loss 4.697045803070068\n",
            "epoch 38, iter 60, loss 0.989658772945404\n",
            "epoch 38, iter 70, loss 0.6478007435798645\n",
            "epoch 38, iter 80, loss 1.8299508094787598\n",
            "epoch 38, iter 90, loss 13.25869369506836\n",
            "epoch 38, iter 100, loss 0.8995746374130249\n",
            "epoch 38, iter 110, loss 1.5020240545272827\n",
            "epoch 38, iter 120, loss 1.1626743078231812\n",
            "epoch 38, iter 130, loss 0.6272914409637451\n",
            "epoch 38, iter 140, loss 1.4800299406051636\n",
            "epoch 38, iter 150, loss 1.4145996570587158\n",
            "epoch 38, iter 160, loss 1.984066367149353\n",
            "epoch 38, iter 170, loss 3.6309967041015625\n",
            "epoch 38, iter 180, loss 0.8141388297080994\n",
            "epoch 38, iter 190, loss 0.3379446566104889\n",
            "epoch 38, iter 200, loss 2.4334285259246826\n",
            "epoch 38, iter 210, loss 1.388911247253418\n",
            "epoch 38, iter 220, loss 0.7271548509597778\n",
            "epoch 38, iter 230, loss 1.6034315824508667\n",
            "epoch 38, iter 240, loss 0.9534811973571777\n",
            "epoch 38, iter 250, loss 1.981592059135437\n",
            "epoch 38, iter 260, loss 0.8717156052589417\n",
            "epoch 38, iter 270, loss 25.726999282836914\n",
            "epoch 38, iter 280, loss 1.7892956733703613\n",
            "epoch 38, iter 290, loss 0.6987040042877197\n",
            "epoch 38, iter 300, loss 0.7842283248901367\n",
            "epoch 38, iter 310, loss 1.6451283693313599\n",
            "epoch 38, iter 320, loss 0.6512232422828674\n",
            "epoch 38, iter 330, loss 0.37203237414360046\n",
            "epoch 38, iter 340, loss 6.493859767913818\n",
            "epoch 39, iter 0, loss 0.5949430465698242\n",
            "epoch 39, iter 10, loss 0.2479531466960907\n",
            "epoch 39, iter 20, loss 15.870927810668945\n",
            "epoch 39, iter 30, loss 1.275598406791687\n",
            "epoch 39, iter 40, loss 0.5812246203422546\n",
            "epoch 39, iter 50, loss 1.0988342761993408\n",
            "epoch 39, iter 60, loss 0.5104469060897827\n",
            "epoch 39, iter 70, loss 1.044339656829834\n",
            "epoch 39, iter 80, loss 0.39554208517074585\n",
            "epoch 39, iter 90, loss 1.643090009689331\n",
            "epoch 39, iter 100, loss 0.7188698053359985\n",
            "epoch 39, iter 110, loss 1.2347571849822998\n",
            "epoch 39, iter 120, loss 0.8015800714492798\n",
            "epoch 39, iter 130, loss 0.6567756533622742\n",
            "epoch 39, iter 140, loss 0.7793875336647034\n",
            "epoch 39, iter 150, loss 0.763745129108429\n",
            "epoch 39, iter 160, loss 0.6121070981025696\n",
            "epoch 39, iter 170, loss 3.868802309036255\n",
            "epoch 39, iter 180, loss 1.8418997526168823\n",
            "epoch 39, iter 190, loss 0.795855700969696\n",
            "epoch 39, iter 200, loss 0.6405883431434631\n",
            "epoch 39, iter 210, loss 0.47253942489624023\n",
            "epoch 39, iter 220, loss 0.6630020141601562\n",
            "epoch 39, iter 230, loss 1.5899749994277954\n",
            "epoch 39, iter 240, loss 0.8147522807121277\n",
            "epoch 39, iter 250, loss 0.7882227301597595\n",
            "epoch 39, iter 260, loss 0.49060675501823425\n",
            "epoch 39, iter 270, loss 1.0655436515808105\n",
            "epoch 39, iter 280, loss 0.9062083959579468\n",
            "epoch 39, iter 290, loss 0.7443836331367493\n",
            "epoch 39, iter 300, loss 1.0192588567733765\n",
            "epoch 39, iter 310, loss 1.1602803468704224\n",
            "epoch 39, iter 320, loss 1.5275824069976807\n",
            "epoch 39, iter 330, loss 1.2002005577087402\n",
            "epoch 39, iter 340, loss 3.8211801052093506\n",
            "epoch 40, iter 0, loss 0.48196133971214294\n",
            "epoch 40, iter 10, loss 1.652725338935852\n",
            "epoch 40, iter 20, loss 0.7020140290260315\n",
            "epoch 40, iter 30, loss 0.49531805515289307\n",
            "epoch 40, iter 40, loss 0.6954385042190552\n",
            "epoch 40, iter 50, loss 11.729886054992676\n",
            "epoch 40, iter 60, loss 1.1050559282302856\n",
            "epoch 40, iter 70, loss 2.3866286277770996\n",
            "epoch 40, iter 80, loss 0.6209391355514526\n",
            "epoch 40, iter 90, loss 0.627661943435669\n",
            "epoch 40, iter 100, loss 16.27684211730957\n",
            "epoch 40, iter 110, loss 1.574397087097168\n",
            "epoch 40, iter 120, loss 0.4119284152984619\n",
            "epoch 40, iter 130, loss 1.4651998281478882\n",
            "epoch 40, iter 140, loss 0.9323714375495911\n",
            "epoch 40, iter 150, loss 0.666787326335907\n",
            "epoch 40, iter 160, loss 1.2070330381393433\n",
            "epoch 40, iter 170, loss 0.9648023247718811\n",
            "epoch 40, iter 180, loss 0.6758294105529785\n",
            "epoch 40, iter 190, loss 0.5771277546882629\n",
            "epoch 40, iter 200, loss 0.5751325488090515\n",
            "epoch 40, iter 210, loss 1.1804876327514648\n",
            "epoch 40, iter 220, loss 1.4901037216186523\n",
            "epoch 40, iter 230, loss 1.2145469188690186\n",
            "epoch 40, iter 240, loss 2.470674514770508\n",
            "epoch 40, iter 250, loss 1.2357457876205444\n",
            "epoch 40, iter 260, loss 1.103056788444519\n",
            "epoch 40, iter 270, loss 0.9475981593132019\n",
            "epoch 40, iter 280, loss 0.41271325945854187\n",
            "epoch 40, iter 290, loss 1.4320604801177979\n",
            "epoch 40, iter 300, loss 0.7283901572227478\n",
            "epoch 40, iter 310, loss 8.493060111999512\n",
            "epoch 40, iter 320, loss 19.152099609375\n",
            "epoch 40, iter 330, loss 3.277841091156006\n",
            "epoch 40, iter 340, loss 0.645404040813446\n",
            "epoch 41, iter 0, loss 1.5363675355911255\n",
            "epoch 41, iter 10, loss 1.3224942684173584\n",
            "epoch 41, iter 20, loss 1.1963754892349243\n",
            "epoch 41, iter 30, loss 0.8453670740127563\n",
            "epoch 41, iter 40, loss 0.758708655834198\n",
            "epoch 41, iter 50, loss 12.409682273864746\n",
            "epoch 41, iter 60, loss 1.27987539768219\n",
            "epoch 41, iter 70, loss 1.3247946500778198\n",
            "epoch 41, iter 80, loss 0.33342069387435913\n",
            "epoch 41, iter 90, loss 1.5259153842926025\n",
            "epoch 41, iter 100, loss 0.886609673500061\n",
            "epoch 41, iter 110, loss 1.070317268371582\n",
            "epoch 41, iter 120, loss 0.7257132530212402\n",
            "epoch 41, iter 130, loss 1.009208083152771\n",
            "epoch 41, iter 140, loss 4.708559989929199\n",
            "epoch 41, iter 150, loss 1.234531044960022\n",
            "epoch 41, iter 160, loss 1.4803611040115356\n",
            "epoch 41, iter 170, loss 4.883077621459961\n",
            "epoch 41, iter 180, loss 0.8962615728378296\n",
            "epoch 41, iter 190, loss 16.62656021118164\n",
            "epoch 41, iter 200, loss 2.4371232986450195\n",
            "epoch 41, iter 210, loss 2.895914077758789\n",
            "epoch 41, iter 220, loss 3.0928704738616943\n",
            "epoch 41, iter 230, loss 1.1031897068023682\n",
            "epoch 41, iter 240, loss 0.72943115234375\n",
            "epoch 41, iter 250, loss 0.8406842947006226\n",
            "epoch 41, iter 260, loss 1.159452199935913\n",
            "epoch 41, iter 270, loss 3.1099305152893066\n",
            "epoch 41, iter 280, loss 1.8713912963867188\n",
            "epoch 41, iter 290, loss 0.5649375915527344\n",
            "epoch 41, iter 300, loss 0.6404879689216614\n",
            "epoch 41, iter 310, loss 1.4807517528533936\n",
            "epoch 41, iter 320, loss 2.651921272277832\n",
            "epoch 41, iter 330, loss 1.9285762310028076\n",
            "epoch 41, iter 340, loss 0.3678125739097595\n",
            "epoch 42, iter 0, loss 0.2141980677843094\n",
            "epoch 42, iter 10, loss 0.4967835545539856\n",
            "epoch 42, iter 20, loss 14.070661544799805\n",
            "epoch 42, iter 30, loss 2.0150346755981445\n",
            "epoch 42, iter 40, loss 0.6261845231056213\n",
            "epoch 42, iter 50, loss 0.6690338253974915\n",
            "epoch 42, iter 60, loss 0.4229438304901123\n",
            "epoch 42, iter 70, loss 0.4420824348926544\n",
            "epoch 42, iter 80, loss 0.7824358344078064\n",
            "epoch 42, iter 90, loss 0.7468449473381042\n",
            "epoch 42, iter 100, loss 1.224301815032959\n",
            "epoch 42, iter 110, loss 0.7766579389572144\n",
            "epoch 42, iter 120, loss 0.29854142665863037\n",
            "epoch 42, iter 130, loss 1.0807173252105713\n",
            "epoch 42, iter 140, loss 1.009647011756897\n",
            "epoch 42, iter 150, loss 0.7747336030006409\n",
            "epoch 42, iter 160, loss 1.0048363208770752\n",
            "epoch 42, iter 170, loss 1.0070185661315918\n",
            "epoch 42, iter 180, loss 0.6827194690704346\n",
            "epoch 42, iter 190, loss 0.6315425634384155\n",
            "epoch 42, iter 200, loss 0.7226096391677856\n",
            "epoch 42, iter 210, loss 2.2767555713653564\n",
            "epoch 42, iter 220, loss 1.6641417741775513\n",
            "epoch 42, iter 230, loss 0.714512288570404\n",
            "epoch 42, iter 240, loss 0.5626704692840576\n",
            "epoch 42, iter 250, loss 0.661160409450531\n",
            "epoch 42, iter 260, loss 1.3542263507843018\n",
            "epoch 42, iter 270, loss 1.24680757522583\n",
            "epoch 42, iter 280, loss 0.42165887355804443\n",
            "epoch 42, iter 290, loss 0.821771502494812\n",
            "epoch 42, iter 300, loss 0.974531352519989\n",
            "epoch 42, iter 310, loss 0.35966309905052185\n",
            "epoch 42, iter 320, loss 3.3946306705474854\n",
            "epoch 42, iter 330, loss 0.6968029737472534\n",
            "epoch 42, iter 340, loss 0.447566419839859\n",
            "epoch 43, iter 0, loss 0.38418373465538025\n",
            "epoch 43, iter 10, loss 0.32656875252723694\n",
            "epoch 43, iter 20, loss 0.7058758735656738\n",
            "epoch 43, iter 30, loss 0.2202560305595398\n",
            "epoch 43, iter 40, loss 0.5960341095924377\n",
            "epoch 43, iter 50, loss 2.292792558670044\n",
            "epoch 43, iter 60, loss 0.762285590171814\n",
            "epoch 43, iter 70, loss 0.5088003873825073\n",
            "epoch 43, iter 80, loss 0.3177953362464905\n",
            "epoch 43, iter 90, loss 1.0896940231323242\n",
            "epoch 43, iter 100, loss 0.39945733547210693\n",
            "epoch 43, iter 110, loss 2.7183854579925537\n",
            "epoch 43, iter 120, loss 0.8376426696777344\n",
            "epoch 43, iter 130, loss 0.7752225399017334\n",
            "epoch 43, iter 140, loss 0.5646889209747314\n",
            "epoch 43, iter 150, loss 2.2137763500213623\n",
            "epoch 43, iter 160, loss 1.486838936805725\n",
            "epoch 43, iter 170, loss 1.0956501960754395\n",
            "epoch 43, iter 180, loss 0.6176760196685791\n",
            "epoch 43, iter 190, loss 5.852184772491455\n",
            "epoch 43, iter 200, loss 0.9006245732307434\n",
            "epoch 43, iter 210, loss 0.7107423543930054\n",
            "epoch 43, iter 220, loss 0.9248148202896118\n",
            "epoch 43, iter 230, loss 0.7725310325622559\n",
            "epoch 43, iter 240, loss 1.2162364721298218\n",
            "epoch 43, iter 250, loss 1.5564650297164917\n",
            "epoch 43, iter 260, loss 0.8444174528121948\n",
            "epoch 43, iter 270, loss 2.2329726219177246\n",
            "epoch 43, iter 280, loss 0.9032054543495178\n",
            "epoch 43, iter 290, loss 0.5876171588897705\n",
            "epoch 43, iter 300, loss 0.8937094211578369\n",
            "epoch 43, iter 310, loss 0.6332383155822754\n",
            "epoch 43, iter 320, loss 1.464110016822815\n",
            "epoch 43, iter 330, loss 0.7362290024757385\n",
            "epoch 43, iter 340, loss 0.8447141051292419\n",
            "epoch 44, iter 0, loss 1.0166707038879395\n",
            "epoch 44, iter 10, loss 1.0377963781356812\n",
            "epoch 44, iter 20, loss 0.7655574083328247\n",
            "epoch 44, iter 30, loss 0.4166898727416992\n",
            "epoch 44, iter 40, loss 0.3364231288433075\n",
            "epoch 44, iter 50, loss 0.2855578064918518\n",
            "epoch 44, iter 60, loss 1.370699405670166\n",
            "epoch 44, iter 70, loss 0.4325944185256958\n",
            "epoch 44, iter 80, loss 0.8240512609481812\n",
            "epoch 44, iter 90, loss 0.6952835917472839\n",
            "epoch 44, iter 100, loss 0.417161226272583\n",
            "epoch 44, iter 110, loss 0.7679080963134766\n",
            "epoch 44, iter 120, loss 0.43850865960121155\n",
            "epoch 44, iter 130, loss 0.7937818169593811\n",
            "epoch 44, iter 140, loss 0.6058574318885803\n",
            "epoch 44, iter 150, loss 1.184470534324646\n",
            "epoch 44, iter 160, loss 0.7685182690620422\n",
            "epoch 44, iter 170, loss 0.8700342178344727\n",
            "epoch 44, iter 180, loss 0.9571896195411682\n",
            "epoch 44, iter 190, loss 3.0357680320739746\n",
            "epoch 44, iter 200, loss 0.7089523077011108\n",
            "epoch 44, iter 210, loss 0.32940900325775146\n",
            "epoch 44, iter 220, loss 0.27350348234176636\n",
            "epoch 44, iter 230, loss 0.3520568907260895\n",
            "epoch 44, iter 240, loss 0.31955963373184204\n",
            "epoch 44, iter 250, loss 12.230847358703613\n",
            "epoch 44, iter 260, loss 0.5154365301132202\n",
            "epoch 44, iter 270, loss 0.44168445467948914\n",
            "epoch 44, iter 280, loss 1.0150939226150513\n",
            "epoch 44, iter 290, loss 1.0757989883422852\n",
            "epoch 44, iter 300, loss 1.7815536260604858\n",
            "epoch 44, iter 310, loss 0.7884162664413452\n",
            "epoch 44, iter 320, loss 2.557513475418091\n",
            "epoch 44, iter 330, loss 2.0500335693359375\n",
            "epoch 44, iter 340, loss 0.7715016007423401\n",
            "epoch 45, iter 0, loss 1.9016833305358887\n",
            "epoch 45, iter 10, loss 0.8731310367584229\n",
            "epoch 45, iter 20, loss 2.0358242988586426\n",
            "epoch 45, iter 30, loss 1.3367822170257568\n",
            "epoch 45, iter 40, loss 0.3892110586166382\n",
            "epoch 45, iter 50, loss 0.5995352864265442\n",
            "epoch 45, iter 60, loss 0.5813592672348022\n",
            "epoch 45, iter 70, loss 1.334411859512329\n",
            "epoch 45, iter 80, loss 0.6796004772186279\n",
            "epoch 45, iter 90, loss 0.6802307963371277\n",
            "epoch 45, iter 100, loss 0.8940352201461792\n",
            "epoch 45, iter 110, loss 0.7872877717018127\n",
            "epoch 45, iter 120, loss 0.4851708710193634\n",
            "epoch 45, iter 130, loss 0.7025583386421204\n",
            "epoch 45, iter 140, loss 2.121227264404297\n",
            "epoch 45, iter 150, loss 0.25377756357192993\n",
            "epoch 45, iter 160, loss 0.3647252023220062\n",
            "epoch 45, iter 170, loss 0.2900613248348236\n",
            "epoch 45, iter 180, loss 0.5124019980430603\n",
            "epoch 45, iter 190, loss 0.5839332342147827\n",
            "epoch 45, iter 200, loss 3.172743797302246\n",
            "epoch 45, iter 210, loss 1.1617571115493774\n",
            "epoch 45, iter 220, loss 0.31251829862594604\n",
            "epoch 45, iter 230, loss 0.3514590263366699\n",
            "epoch 45, iter 240, loss 0.2461891770362854\n",
            "epoch 45, iter 250, loss 0.2985389232635498\n",
            "epoch 45, iter 260, loss 0.3349092900753021\n",
            "epoch 45, iter 270, loss 0.5251071453094482\n",
            "epoch 45, iter 280, loss 0.3139372169971466\n",
            "epoch 45, iter 290, loss 0.7138476967811584\n",
            "epoch 45, iter 300, loss 1.282758355140686\n",
            "epoch 45, iter 310, loss 1.1242930889129639\n",
            "epoch 45, iter 320, loss 0.393066942691803\n",
            "epoch 45, iter 330, loss 0.4431147873401642\n",
            "epoch 45, iter 340, loss 0.5682578682899475\n",
            "epoch 46, iter 0, loss 0.4000557065010071\n",
            "epoch 46, iter 10, loss 0.5176922678947449\n",
            "epoch 46, iter 20, loss 0.323709100484848\n",
            "epoch 46, iter 30, loss 0.42635515332221985\n",
            "epoch 46, iter 40, loss 0.8464817404747009\n",
            "epoch 46, iter 50, loss 0.25870874524116516\n",
            "epoch 46, iter 60, loss 0.2975774109363556\n",
            "epoch 46, iter 70, loss 0.6402738094329834\n",
            "epoch 46, iter 80, loss 0.8454084992408752\n",
            "epoch 46, iter 90, loss 2.0850307941436768\n",
            "epoch 46, iter 100, loss 0.5947186350822449\n",
            "epoch 46, iter 110, loss 1.9627759456634521\n",
            "epoch 46, iter 120, loss 0.5372328162193298\n",
            "epoch 46, iter 130, loss 0.7415240406990051\n",
            "epoch 46, iter 140, loss 0.4596424698829651\n",
            "epoch 46, iter 150, loss 0.44572803378105164\n",
            "epoch 46, iter 160, loss 0.5589795112609863\n",
            "epoch 46, iter 170, loss 8.556106567382812\n",
            "epoch 46, iter 180, loss 1.395125389099121\n",
            "epoch 46, iter 190, loss 1.2863057851791382\n",
            "epoch 46, iter 200, loss 21.699207305908203\n",
            "epoch 46, iter 210, loss 1.9791401624679565\n",
            "epoch 46, iter 220, loss 2.4350688457489014\n",
            "epoch 46, iter 230, loss 1.3602852821350098\n",
            "epoch 46, iter 240, loss 0.9416694641113281\n",
            "epoch 46, iter 250, loss 0.416511595249176\n",
            "epoch 46, iter 260, loss 0.817258358001709\n",
            "epoch 46, iter 270, loss 0.3577727675437927\n",
            "epoch 46, iter 280, loss 1.4015891551971436\n",
            "epoch 46, iter 290, loss 0.3900621831417084\n",
            "epoch 46, iter 300, loss 0.3701344132423401\n",
            "epoch 46, iter 310, loss 0.4040473699569702\n",
            "epoch 46, iter 320, loss 1.3697797060012817\n",
            "epoch 46, iter 330, loss 0.4385756850242615\n",
            "epoch 46, iter 340, loss 0.5737133026123047\n",
            "epoch 47, iter 0, loss 0.5381876230239868\n",
            "epoch 47, iter 10, loss 0.5593243837356567\n",
            "epoch 47, iter 20, loss 0.6854363679885864\n",
            "epoch 47, iter 30, loss 0.19277553260326385\n",
            "epoch 47, iter 40, loss 0.2651684582233429\n",
            "epoch 47, iter 50, loss 0.48891395330429077\n",
            "epoch 47, iter 60, loss 7.7176008224487305\n",
            "epoch 47, iter 70, loss 0.669539213180542\n",
            "epoch 47, iter 80, loss 0.6289336085319519\n",
            "epoch 47, iter 90, loss 1.07293701171875\n",
            "epoch 47, iter 100, loss 0.32912537455558777\n",
            "epoch 47, iter 110, loss 0.47363418340682983\n",
            "epoch 47, iter 120, loss 0.24317438900470734\n",
            "epoch 47, iter 130, loss 0.17537380754947662\n",
            "epoch 47, iter 140, loss 0.5140515565872192\n",
            "epoch 47, iter 150, loss 1.0877339839935303\n",
            "epoch 47, iter 160, loss 0.8146922588348389\n",
            "epoch 47, iter 170, loss 0.8647646307945251\n",
            "epoch 47, iter 180, loss 0.30594950914382935\n",
            "epoch 47, iter 190, loss 0.2099224478006363\n",
            "epoch 47, iter 200, loss 0.2764785885810852\n",
            "epoch 47, iter 210, loss 1.670837640762329\n",
            "epoch 47, iter 220, loss 0.9693986177444458\n",
            "epoch 47, iter 230, loss 0.946698009967804\n",
            "epoch 47, iter 240, loss 0.18228156864643097\n",
            "epoch 47, iter 250, loss 0.49726057052612305\n",
            "epoch 47, iter 260, loss 0.8687072396278381\n",
            "epoch 47, iter 270, loss 1.8549867868423462\n",
            "epoch 47, iter 280, loss 2.1426265239715576\n",
            "epoch 47, iter 290, loss 0.4187142848968506\n",
            "epoch 47, iter 300, loss 0.5545117855072021\n",
            "epoch 47, iter 310, loss 0.5891206860542297\n",
            "epoch 47, iter 320, loss 0.5154179334640503\n",
            "epoch 47, iter 330, loss 0.6468530893325806\n",
            "epoch 47, iter 340, loss 0.7511958479881287\n",
            "epoch 48, iter 0, loss 0.6027535796165466\n",
            "epoch 48, iter 10, loss 16.207927703857422\n",
            "epoch 48, iter 20, loss 1.4527171850204468\n",
            "epoch 48, iter 30, loss 0.8769057989120483\n",
            "epoch 48, iter 40, loss 2.020660877227783\n",
            "epoch 48, iter 50, loss 2.1961684226989746\n",
            "epoch 48, iter 60, loss 0.9344891309738159\n",
            "epoch 48, iter 70, loss 0.4915355145931244\n",
            "epoch 48, iter 80, loss 4.522922039031982\n",
            "epoch 48, iter 90, loss 0.46695175766944885\n",
            "epoch 48, iter 100, loss 1.3041479587554932\n",
            "epoch 48, iter 110, loss 1.5732626914978027\n",
            "epoch 48, iter 120, loss 0.3869814872741699\n",
            "epoch 48, iter 130, loss 0.35657215118408203\n",
            "epoch 48, iter 140, loss 0.33142074942588806\n",
            "epoch 48, iter 150, loss 0.26598548889160156\n",
            "epoch 48, iter 160, loss 0.4710858464241028\n",
            "epoch 48, iter 170, loss 0.5994387865066528\n",
            "epoch 48, iter 180, loss 0.3109853267669678\n",
            "epoch 48, iter 190, loss 0.1783926635980606\n",
            "epoch 48, iter 200, loss 0.929322361946106\n",
            "epoch 48, iter 210, loss 1.343592882156372\n",
            "epoch 48, iter 220, loss 0.3063753843307495\n",
            "epoch 48, iter 230, loss 0.42163872718811035\n",
            "epoch 48, iter 240, loss 0.6128257513046265\n",
            "epoch 48, iter 250, loss 0.3900706171989441\n",
            "epoch 48, iter 260, loss 0.7166788578033447\n",
            "epoch 48, iter 270, loss 0.4915253818035126\n",
            "epoch 48, iter 280, loss 0.7596351504325867\n",
            "epoch 48, iter 290, loss 0.7180672883987427\n",
            "epoch 48, iter 300, loss 0.40115678310394287\n",
            "epoch 48, iter 310, loss 1.1123766899108887\n",
            "epoch 48, iter 320, loss 0.5136536955833435\n",
            "epoch 48, iter 330, loss 0.574420154094696\n",
            "epoch 48, iter 340, loss 0.6469675302505493\n",
            "epoch 49, iter 0, loss 1.1546040773391724\n",
            "epoch 49, iter 10, loss 0.9289442896842957\n",
            "epoch 49, iter 20, loss 0.597652792930603\n",
            "epoch 49, iter 30, loss 0.3701570928096771\n",
            "epoch 49, iter 40, loss 0.2084266096353531\n",
            "epoch 49, iter 50, loss 0.29630589485168457\n",
            "epoch 49, iter 60, loss 0.749467670917511\n",
            "epoch 49, iter 70, loss 0.5657116770744324\n",
            "epoch 49, iter 80, loss 13.09872055053711\n",
            "epoch 49, iter 90, loss 0.4807443916797638\n",
            "epoch 49, iter 100, loss 0.44721630215644836\n",
            "epoch 49, iter 110, loss 0.3725891709327698\n",
            "epoch 49, iter 120, loss 1.1555538177490234\n",
            "epoch 49, iter 130, loss 0.39034828543663025\n",
            "epoch 49, iter 140, loss 0.4203207492828369\n",
            "epoch 49, iter 150, loss 0.4299553632736206\n",
            "epoch 49, iter 160, loss 0.17266349494457245\n",
            "epoch 49, iter 170, loss 0.7816503047943115\n",
            "epoch 49, iter 180, loss 0.5121858716011047\n",
            "epoch 49, iter 190, loss 5.0861496925354\n",
            "epoch 49, iter 200, loss 0.39707151055336\n",
            "epoch 49, iter 210, loss 1.0657247304916382\n",
            "epoch 49, iter 220, loss 0.7804157733917236\n",
            "epoch 49, iter 230, loss 0.7141069769859314\n",
            "epoch 49, iter 240, loss 0.5328109860420227\n",
            "epoch 49, iter 250, loss 0.4882413148880005\n",
            "epoch 49, iter 260, loss 0.6087262034416199\n",
            "epoch 49, iter 270, loss 1.0395997762680054\n",
            "epoch 49, iter 280, loss 0.3633151352405548\n",
            "epoch 49, iter 290, loss 0.6958664655685425\n",
            "epoch 49, iter 300, loss 1.4721177816390991\n",
            "epoch 49, iter 310, loss 0.9031702280044556\n",
            "epoch 49, iter 320, loss 0.5645039677619934\n",
            "epoch 49, iter 330, loss 0.5845208168029785\n",
            "epoch 49, iter 340, loss 0.5672307014465332\n",
            "Training completed and model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('keypoints_model.pth'))\n",
        "optimizer.load_state_dict(torch.load('optimizer.pth'))\n",
        "\n",
        "# Define the number of additional epochs\n",
        "additional_epochs = 20\n",
        "total_epochs = epochs + additional_epochs  # Total epochs including previously trained epochs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs, total_epochs):\n",
        "    for i, (imgs, kps) in enumerate(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        kps = kps.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, kps)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"epoch {epoch}, iter {i}, loss {loss.item()}\")\n",
        "\n",
        "# Save the model again after training\n",
        "torch.save(model.state_dict(), 'final_model.pth')\n",
        "print(\"Training completed and model saved successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}